{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📚 A First Lesson in Machine Learning\n",
        "## Comprehensive Study Guide: Bayesian Modeling & Data Science\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 **Course Overview**\n",
        "This study guide covers the essential concepts from \"A First Lesson in Machine Learning,\" focusing on Bayesian approaches to machine learning, data preparation, and visualization techniques.\n",
        "\n",
        "---\n",
        "\n",
        "### 📋 **Table of Contents**\n",
        "\n",
        "#### **Part I: Bayesian Foundations**\n",
        "1. [Chapter 3.1-3.3: The Coin Game - Introduction to Bayesian Thinking](#coin-game)\n",
        "2. [Chapter 3.4-3.9: Advanced Bayesian Inference](#bayesian-inference)\n",
        "3. [Chapter 4.1-4.5: Approximation Methods & Sampling](#approximation-methods)\n",
        "\n",
        "#### **Part II: Data Science Pipeline**\n",
        "4. [Data Wrangling: From Raw Data to Analysis-Ready](#data-wrangling)\n",
        "5. [Data Visualization: Making Data Tell Stories](#data-visualization)\n",
        "\n",
        "#### **Part III: Integration & Practice**\n",
        "6. [Key Takeaways & Professional Practices](#key-takeaways)\n",
        "7. [Quick Checks & Practice Problems](#practice-problems)\n",
        "\n",
        "---\n",
        "Important Links!\n",
        "\n",
        "- [Github Repo](https://github.com/omniV1/GCU_SWE_2023-2025/tree/main/AIT-104-Data-mining-machine-learning/Notes)\n",
        "- [Google Colab](https://colab.research.google.com/drive/1Z2TcxA_jAEkEwfusNpaPwKJ8ow01bx38)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part I: Bayesian Foundations\n",
        "\n",
        "## 1. Chapter 3.1-3.3: The Coin Game - Introduction to Bayesian Thinking\n",
        "\n",
        "### 🎯 **What Makes Bayesian Statistics Special?**\n",
        "\n",
        "**Traditional Statistics vs. Bayesian:**\n",
        "- **Traditional:** Parameters are fixed, unknown values we estimate\n",
        "- **Bayesian:** Parameters are random variables with probability distributions\n",
        "\n",
        "**Why This Matters:**\n",
        "- We can express and update our uncertainty\n",
        "- We incorporate prior knowledge\n",
        "- We get probability distributions for predictions, not just point estimates\n",
        "\n",
        "### 🎲 **The Coin Game: Perfect Introduction to Bayesian Thinking**\n",
        "\n",
        "**The Setup:**\n",
        "```\n",
        "Fairground stall game:\n",
        "- Toss coin 10 times per customer\n",
        "- Customer pays £1 to play\n",
        "- If ≤6 heads: customer wins £2 (£1 profit)\n",
        "- If >6 heads: stall owner keeps £1\n",
        "\n",
        "Question: Should you play?\n",
        "```\n",
        "\n",
        "**Step 1: Traditional Analysis (Assuming Fair Coin)**\n",
        "```\n",
        "If r = 0.5 (fair coin):\n",
        "P(winning) = P(≤6 heads) = 0.8281\n",
        "Expected return = £1.66 per game\n",
        "Expected profit = 66p per game\n",
        "```\n",
        "\n",
        "**The Problem:** What if our assumption is wrong?\n",
        "\n",
        "**Step 2: The Bayesian Approach - What if r is Unknown?**\n",
        "\n",
        "Instead of assuming r = 0.5, let's treat r as a random variable R with a probability distribution.\n",
        "\n",
        "### 🔑 **Bayes' Rule: The Foundation**\n",
        "\n",
        "**The Famous Formula:**\n",
        "```\n",
        "P(r|data) = P(data|r) × P(r) / P(data)\n",
        "\n",
        "Where:\n",
        "- P(r|data) = Posterior (what we want)\n",
        "- P(data|r) = Likelihood (how likely is our data given r?)\n",
        "- P(r) = Prior (what did we believe before seeing data?)\n",
        "- P(data) = Marginal likelihood (normalizing constant)\n",
        "```\n",
        "\n",
        "**In Plain English:**\n",
        "\"Our updated belief about r = (How well r explains our data) × (Our initial belief) / (Total probability of seeing this data)\"\n",
        "\n",
        "### 📊 **The Three Key Components Explained**\n",
        "\n",
        "**1. Likelihood P(data|r):**\n",
        "- How probable is our observed data for a given value of r?\n",
        "- For coin tosses: Binomial distribution\n",
        "- **Clarification:** This is NOT a probability density - it doesn't integrate to 1!\n",
        "\n",
        "**2. Prior P(r):**\n",
        "- What did we believe about r before seeing any data?\n",
        "- Three scenarios in the text:\n",
        "  - **Scenario 1:** No knowledge (Uniform: α=1, β=1)\n",
        "  - **Scenario 2:** Fair coin belief (Concentrated: α=50, β=50)\n",
        "  - **Scenario 3:** Biased coin belief (Skewed: α=5, β=1)\n",
        "\n",
        "**3. Posterior P(r|data):**\n",
        "- Our updated belief after seeing the data\n",
        "- This is what we use for making decisions!\n",
        "\n",
        "### 🧮 **Beta-Binomial Conjugacy: Mathematical Magic**\n",
        "\n",
        "**Why Beta Distribution for Prior?**\n",
        "- Beta distribution is \"conjugate\" to binomial likelihood\n",
        "- This means: Beta prior + Binomial likelihood = Beta posterior\n",
        "- **Huge advantage:** We can calculate everything analytically!\n",
        "\n",
        "**The Update Rule:**\n",
        "```\n",
        "If prior is Beta(α, β) and we see y heads in N tosses:\n",
        "Posterior is Beta(α + y, β + N - y)\n",
        "```\n",
        "\n",
        "**Intuitive Interpretation:**\n",
        "- α = \"virtual heads\" from prior experience\n",
        "- β = \"virtual tails\" from prior experience\n",
        "- We simply add real observations to virtual ones!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 📈 **The Learning Process: How Beliefs Evolve**\n",
        "\n",
        "#### **Scenario 1: No Prior Knowledge (α=1, β=1)**\n",
        "\n",
        "**Starting beliefs:** All values of r equally likely\n",
        "\n",
        "```\n",
        "Observed sequence: H, T, H, H, H, T, H, H, T, H\n",
        "Final result: 6 heads, 4 tails in 10 tosses\n",
        "\n",
        "Posterior: Beta(1+6=7, 1+4=5)\n",
        "Expected r = 7/12 ≈ 0.583\n",
        "Expected P(winning) ≈ 0.551\n",
        "```\n",
        "\n",
        "> **🔑 Key Learning:** Each observation has significant impact when we start with weak beliefs.\n",
        "\n",
        "#### **Scenario 2: Strong Fair Coin Belief (α=50, β=50)**\n",
        "\n",
        "**Starting beliefs:** Very confident coin is fair (equivalent to 100 previous tosses)\n",
        "\n",
        "```\n",
        "Same observed data: 6 heads, 4 tails\n",
        "\n",
        "Posterior: Beta(50+6=56, 50+4=54)  \n",
        "Expected r = 56/110 ≈ 0.509\n",
        "Expected P(winning) ≈ 0.793\n",
        "```\n",
        "\n",
        "> **🔑 Key Learning:** Strong priors are resistant to change - need lots of data to overcome them.\n",
        "\n",
        "#### **Scenario 3: Biased Coin Belief (α=5, β=1)**\n",
        "\n",
        "**Starting beliefs:** Coin probably biased toward heads\n",
        "\n",
        "```\n",
        "Same observed data: 6 heads, 4 tails\n",
        "\n",
        "Posterior: Beta(5+6=11, 1+4=5)\n",
        "Expected r = 11/16 ≈ 0.688\n",
        "Expected P(winning) ≈ 0.235\n",
        "```\n",
        "\n",
        "> **🔑 Key Learning:** Different priors lead to very different conclusions from the same data!\n",
        "\n",
        "### 🎯 **Bayesian Decision Making**\n",
        "\n",
        "**The Power of Expectations:**\n",
        "Instead of using point estimates, we calculate:\n",
        "```\n",
        "E[P(winning)] = ∫ P(≤6 heads | r) × P(r|data) dr\n",
        "```\n",
        "\n",
        "This gives us the **expected probability** of winning, accounting for our uncertainty about r.\n",
        "\n",
        "**Why This Matters:**\n",
        "- Point estimates ignore uncertainty\n",
        "- Bayesian approach is more conservative and realistic\n",
        "- Better for high-stakes decisions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coin evidence (Beta-Binomial) and posterior demonstration\n",
        "import numpy as np\n",
        "from scipy.stats import beta, binom\n",
        "from math import comb\n",
        "\n",
        "# Data: N tosses, y heads\n",
        "N = 20\n",
        "y = 14\n",
        "\n",
        "# Priors to compare\n",
        "priors = {\n",
        "    'Uniform Beta(1,1)': (1, 1),\n",
        "    'Fair Beta(50,50)': (50, 50),\n",
        "    'Biased Beta(5,1)': (5, 1),\n",
        "}\n",
        "\n",
        "# Evidence for Beta-Binomial has closed form via Beta functions:\n",
        "# p(y|N, α, β) = C(N,y) * B(α+y, β+N−y) / B(α,β)\n",
        "from scipy.special import betaln\n",
        "\n",
        "def log_evidence(N, y, a, b):\n",
        "    return np.log(comb(N, y)) + betaln(a + y, b + N - y) - betaln(a, b)\n",
        "\n",
        "for name, (a, b) in priors.items():\n",
        "    le = log_evidence(N, y, a, b)\n",
        "    print(f\"{name}: log evidence = {le:.4f}, evidence = {np.exp(le):.6f}\")\n",
        "\n",
        "# Posterior parameters under Beta prior\n",
        "# posterior: Beta(α+y, β+N−y)\n",
        "name = 'Uniform Beta(1,1)'\n",
        "a, b = priors[name]\n",
        "post_a, post_b = a + y, b + (N - y)\n",
        "print(f\"\\nPosterior for {name}: Beta({post_a},{post_b})\\nMean r = {post_a/(post_a+post_b):.4f}\")\n",
        "\n",
        "# Posterior predictive probability of ≤6 heads in 10 tosses:\n",
        "# E_r[ P(Y<=6 | r) ] approximated by Monte Carlo sampling r ~ Beta(post_a, post_b)\n",
        "import random\n",
        "T = 20000\n",
        "rs = beta.rvs(post_a, post_b, size=T, random_state=42)\n",
        "\n",
        "# For each r, compute binomial CDF for 10 tosses\n",
        "from scipy.stats import binom\n",
        "cdf_vals = binom.cdf(6, 10, rs)\n",
        "print(f\"E[P(Y<=6 | r)] ≈ {cdf_vals.mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Chapter 3.4-3.9: Advanced Bayesian Inference\n",
        "\n",
        "### 📊 **3.4 Marginal Likelihoods (Evidence) and Model Comparison**\n",
        "\n",
        "**Definition:** The marginal likelihood (evidence) averages the likelihood over the prior: \n",
        "\n",
        "```math\n",
        "p(y_N|\\alpha,\\beta) = \\int p(y_N|r)\\,p(r|\\alpha,\\beta)\\,dr\n",
        "```\n",
        "\n",
        "**Interpretation:** \"How compatible are the observed data and the prior?\" Higher values indicate the prior/model explains the data better.\n",
        "\n",
        "**Analytic in the coin example:** With Beta–Binomial, the integral reduces to a Beta function ratio, so `p(y_N|α,β)` is available in closed form.\n",
        "\n",
        "**Use cases:**\n",
        "- Compare prior settings (e.g., uniform vs. fair vs. biased coin)\n",
        "- Compare different model families (e.g., polynomial orders in regression)\n",
        "\n",
        "**Type II ML (Empirical Bayes):** Choose prior hyperparameters that maximize evidence. Powerful but can undermine \"prior as belief.\"\n",
        "\n",
        "> **⚠️ Pitfalls:**\n",
        "> - Evidence is sensitive to prior support/scale; overly tight priors can unduly penalize good models\n",
        "> - Hard to compute in non-conjugate models (often requires approximation)\n",
        "\n",
        "### 🏗️ **3.5 Hyperparameters and Hierarchical Bayes**\n",
        "\n",
        "**Motivation:** We often don't know good values for prior parameters (e.g., `α,β`).\n",
        "\n",
        "**Approach:** Place priors on priors (hyperpriors), creating a hierarchy with joint posterior over all unknowns.\n",
        "\n",
        "```math\n",
        "p(r,\\alpha,\\beta|y_N,\\kappa) \\propto p(y_N|r)\\,p(r|\\alpha,\\beta)\\,p(\\alpha|\\kappa)\\,p(\\beta|\\kappa)\n",
        "```\n",
        "\n",
        "**Pros:** Shares strength across groups, captures uncertainty in hyperparameters.\n",
        "\n",
        "**Cons:** Inference becomes more complex; often needs MCMC or variational methods.\n",
        "\n",
        "### 📊 **3.6 Graphical Models (Probabilistic Graphs)**\n",
        "\n",
        "**What they are:** Directed or undirected graphs encoding conditional independencies.\n",
        "\n",
        "**Why useful:**\n",
        "- Clarify assumptions and factorization\n",
        "- Guide efficient inference algorithms\n",
        "\n",
        "**Notation:**\n",
        "- Nodes = random variables, shaded = observed\n",
        "- Plates = repetition over index sets (e.g., data points)\n",
        "\n",
        "### 🏃 **3.8 Bayesian Linear Regression (Olympic 100m)**\n",
        "\n",
        "**Model:** \n",
        "\n",
        "```math\n",
        "t|X,w,\\sigma^2 \\sim \\mathcal{N}(Xw,\\sigma^2 I)\n",
        "```\n",
        "\n",
        "with prior \n",
        "\n",
        "```math\n",
        "w \\sim \\mathcal{N}(\\mu_0,\\Sigma_0)\n",
        "```\n",
        "\n",
        "**Posterior:** Gaussian via completing the square:\n",
        "\n",
        "```math\n",
        "\\Sigma_w = (\\Sigma_0^{-1} + \\sigma^{-2} X^T X)^{-1}\n",
        "```\n",
        "\n",
        "```math\n",
        "\\mu_w = \\Sigma_w(\\Sigma_0^{-1}\\mu_0 + \\sigma^{-2} X^T t)\n",
        "```\n",
        "\n",
        "**MAP and Ridge:** If `μ₀=0` and `Σ₀=λ⁻¹I`, `μ_w` matches ridge regression estimator.\n",
        "\n",
        "**Predictive distribution:** For new `x_new`:\n",
        "\n",
        "```math\n",
        "p(t_{new}|x_{new},\\mathcal{D}) = \\mathcal{N}(x_{new}^T\\mu_w,\\; x_{new}^T\\Sigma_w x_{new} + \\sigma^2)\n",
        "```\n",
        "\n",
        "- Mean uses posterior mean parameters; variance adds parameter uncertainty + noise\n",
        "\n",
        "> **💡 Insights:** Averaging over `w` yields realistic uncertainty bands vs. single-point fits.\n",
        "\n",
        "### 🎯 **3.9 Model Order Selection via Marginal Likelihood**\n",
        "\n",
        "**Setup:** Compete models `M_k` (e.g., polynomial degree `k`) with priors on `w`.\n",
        "\n",
        "**Criterion:** Choose `k` maximizing `p(t|X,M_k)` (closed-form for conjugate linear-Gaussian).\n",
        "\n",
        "**Effect of prior scale:** Smaller prior variance (stronger shrinkage) penalizes large coefficients, can favor simpler functions unless data strongly supports complexity.\n",
        "\n",
        "**When to prefer CV:** If evidence is hard to compute or priors are poorly specified, K-fold CV may be more robust.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bayesian Linear Regression with predictive intervals\n",
        "import numpy as np\n",
        "from numpy.linalg import inv\n",
        "\n",
        "# Synthetic data\n",
        "def make_poly_features(x, degree=1):\n",
        "    X = np.vstack([x**d for d in range(degree+1)]).T\n",
        "    return X\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "x = np.linspace(-3, 3, 60)\n",
        "X = make_poly_features(x, degree=1)\n",
        "w_true = np.array([1.0, 2.0])\n",
        "sigma2 = 0.3**2\n",
        "y = X @ w_true + rng.normal(0, np.sqrt(sigma2), size=len(x))\n",
        "\n",
        "# Prior w ~ N(mu0, Sigma0)\n",
        "mu0 = np.zeros(X.shape[1])\n",
        "Sigma0 = np.eye(X.shape[1]) * 1.0\n",
        "\n",
        "# Posterior\n",
        "Sigma_w = inv(inv(Sigma0) + (X.T @ X) / sigma2)\n",
        "mu_w = Sigma_w @ (inv(Sigma0) @ mu0 + (X.T @ y) / sigma2)\n",
        "\n",
        "# Predictive for new points\n",
        "x_new = np.linspace(-3.5, 3.5, 200)\n",
        "X_new = make_poly_features(x_new, degree=1)\n",
        "mean_pred = X_new @ mu_w\n",
        "var_pred = np.sum(X_new @ Sigma_w * X_new, axis=1) + sigma2\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.scatter(x, y, s=15, alpha=0.6, label='data')\n",
        "plt.plot(x_new, mean_pred, color='C1', label='predictive mean')\n",
        "plt.fill_between(x_new, mean_pred-2*np.sqrt(var_pred), mean_pred+2*np.sqrt(var_pred), \n",
        "                 color='C1', alpha=0.2, label='~95% interval')\n",
        "plt.legend(); plt.title('Bayesian Linear Regression predictive'); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 4: Bayesian Inference — Approximations & Sampling (A First Lesson)\n",
        "\n",
        "## 4.1 Non‑conjugate Models: When Closed Forms Don't Exist\n",
        "\n",
        "> **The Core Challenge**: In Chapter 3, we discovered the elegant beauty of conjugate pairs like Beta-Binomial, where mathematical magic happens and everything works out perfectly. But here's the reality check: most real-world problems don't play by these rules.\n",
        "\n",
        "### The Mathematical Reality\n",
        "\n",
        "When our prior and likelihood don't form a conjugate pair, the posterior `p(w|D)` becomes **intractable**—we can't write down a simple formula for it. This isn't a failure of our understanding; it's the mathematical reality of complex models.\n",
        "\n",
        "### Why This Matters in Practice\n",
        "\n",
        "| **Conjugate Models** | **Non-Conjugate Models** |\n",
        "|---------------------|-------------------------|\n",
        "| ✅ Closed-form solutions | ❌ No closed form |\n",
        "| ✅ Easy to compute | ❌ Requires approximation |\n",
        "| ✅ Limited to simple cases | ✅ Real-world complexity |\n",
        "| Examples: Beta-Binomial, Normal-Normal | Examples: Logistic regression, Neural networks |\n",
        "\n",
        "**The Bottom Line**: Most practical machine learning models are non-conjugate, which means we need sophisticated approximation methods.\n",
        "\n",
        "### Three Strategic Approaches\n",
        "\n",
        "#### 🚀 **Maximum A Posteriori (MAP)**\n",
        "> *\"Find the peak and call it a day\"*\n",
        "\n",
        "**Philosophy**: Instead of capturing the full posterior distribution, find the single parameter value that maximizes the posterior probability.\n",
        "\n",
        "**Characteristics**:\n",
        "- ⚡ **Speed**: Lightning fast computation\n",
        "- 🎯 **Focus**: Single best estimate\n",
        "- ❌ **Limitation**: No uncertainty quantification\n",
        "\n",
        "**Best For**: Quick prototyping, when you only need point estimates, initial model exploration\n",
        "\n",
        "#### 🎯 **Laplace Approximation**\n",
        "> *\"Approximate the mountain as a hill\"*\n",
        "\n",
        "**Philosophy**: Even though the true posterior isn't Gaussian, we can approximate it as Gaussian near the MAP estimate.\n",
        "\n",
        "**Characteristics**:\n",
        "- ⚡ **Speed**: Much faster than MCMC\n",
        "- 📊 **Uncertainty**: Captures local uncertainty around the mode\n",
        "- ⚠️ **Assumption**: Assumes Gaussian shape near mode\n",
        "\n",
        "**Best For**: When you need uncertainty but the posterior is roughly Gaussian near the mode\n",
        "\n",
        "#### 🔄 **Metropolis-Hastings (MCMC)**\n",
        "> *\"Sample the true landscape, no approximations\"*\n",
        "\n",
        "**Philosophy**: Generate samples from the true posterior, even when we can't write it down analytically.\n",
        "\n",
        "**Characteristics**:\n",
        "- 🎯 **Accuracy**: Samples from true posterior\n",
        "- 📊 **Uncertainty**: Captures full posterior uncertainty\n",
        "- ⏱️ **Cost**: Computationally expensive\n",
        "\n",
        "**Best For**: When you need full posterior uncertainty and can afford computational cost\n",
        "\n",
        "### Decision Framework\n",
        "\n",
        "### Decision Framework (no diagram)\n",
        "\n",
        "- If you need no uncertainty: use MAP for a fast point estimate (great for prototyping).\n",
        "- If local uncertainty is sufficient: use Laplace (Gaussian around MAP).\n",
        "- If you need full posterior uncertainty: use MCMC (e.g., Metropolis–Hastings).\n",
        "\n",
        "Examples:\n",
        "- Quick prototyping → MAP\n",
        "- Moderate complexity with unimodal posterior → Laplace\n",
        "- High-stakes or multimodal/complex posteriors → MCMC\n",
        "\n",
        "## 4.2 Binary Response Setup: Logistic Regression Foundation\n",
        "\n",
        "### The Mathematical Foundation\n",
        "\n",
        "> **The Challenge**: How do we model binary outcomes (yes/no, success/failure, 0/1) when our predictors can take any real value?\n",
        "\n",
        "#### The Logistic Transformation\n",
        "\n",
        "The logistic function elegantly solves this problem by mapping any real number to the probability interval (0,1):\n",
        "\n",
        "```math\n",
        "p(t_n=1\\mid x_n,w)=\\sigma(w^T x_n),\\quad p(t_n=0\\mid x_n,w)=1-\\sigma(w^T x_n)\n",
        "```\n",
        "\n",
        "where the sigmoid function creates the smooth S-curve:\n",
        "```math\n",
        "\\sigma(z)=\\frac{1}{1+e^{-z}}\n",
        "```\n",
        "\n",
        "#### Why This Mathematical Choice?\n",
        "\n",
        "| **Property** | **Why It Matters** |\n",
        "|--------------|-------------------|\n",
        "| **Smooth Mapping** | Maps any real number to (0,1) probability |\n",
        "| **Differentiable** | Enables gradient-based optimization |\n",
        "| **Monotonic** | Larger inputs → higher probabilities |\n",
        "| **Symmetric** | Balanced around zero |\n",
        "\n",
        "#### The Prior Belief System\n",
        "\n",
        "We encode our beliefs about reasonable parameter values through a Gaussian prior:\n",
        "\n",
        "```math\n",
        "w \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)\n",
        "```\n",
        "\n",
        "**Common Prior Choices**:\n",
        "\n",
        "| **Prior Type** | **Parameters** | **Interpretation** |\n",
        "|---------------|----------------|-------------------|\n",
        "| **Weak Prior** | `μ₀ = 0`, `Σ₀ = τ²I` (large τ) | \"I don't know much, let data decide\" |\n",
        "| **Strong Prior** | `μ₀ = 0`, `Σ₀ = τ²I` (small τ) | \"Parameters should be small\" |\n",
        "| **Informative Prior** | `μ₀ ≠ 0`, specific `Σ₀` | \"I have domain knowledge\" |\n",
        "\n",
        "> **Key Insight**: The prior acts as a \"regularization\" mechanism, preventing overfitting by keeping parameters reasonable.\n",
        "\n",
        "## 4.3 MAP Estimation: Finding the Peak with Newton-Raphson\n",
        "\n",
        "### The Optimization Challenge\n",
        "\n",
        "> **The Mission**: Find the parameter values that maximize the posterior probability, transforming our Bayesian problem into an optimization problem.\n",
        "\n",
        "#### The Mathematical Objective\n",
        "\n",
        "We seek the parameter vector that maximizes the log posterior:\n",
        "\n",
        "```math\n",
        "\\hat{w}_{MAP}=\\arg\\max_w \\{\\log p(t\\mid X,w)+\\log p(w)\\}\n",
        "```\n",
        "\n",
        "> **Why Log Probabilities?** \n",
        "> - **Numerical Stability**: Prevents underflow with tiny probabilities\n",
        "> - **Computational Efficiency**: Products become sums\n",
        "> - **Mathematical Convenience**: Derivatives are cleaner\n",
        "\n",
        "#### The Newton-Raphson Algorithm\n",
        "\n",
        "This sophisticated optimization method uses **second-order information** (curvature) for faster convergence than simple gradient descent.\n",
        "\n",
        "##### The Update Mechanism\n",
        "\n",
        "```math\n",
        "w \\leftarrow w - H^{-1}\\nabla\n",
        "```\n",
        "\n",
        "**Component Breakdown**:\n",
        "\n",
        "| **Component** | **Role** | **Mathematical Meaning** |\n",
        "|---------------|----------|-------------------------|\n",
        "| `∇` (Gradient) | First derivatives | Direction of steepest ascent |\n",
        "| `H` (Hessian) | Second derivatives | Curvature information |\n",
        "| `H⁻¹∇` | Optimal step | Direction + size in one calculation |\n",
        "\n",
        "##### The Gradient Components\n",
        "\n",
        "```math\n",
        "\\nabla=X^T(\\hat p-t)+\\Sigma_0^{-1}(w-\\mu_0)\n",
        "```\n",
        "\n",
        "**Interpretation**:\n",
        "\n",
        "| **Term** | **Meaning** | **Intuition** |\n",
        "|----------|-------------|---------------|\n",
        "| `X^T(̂p-t)` | Data-driven correction | \"How much do my predictions differ from reality?\" |\n",
        "| `Σ₀⁻¹(w-μ₀)` | Prior penalty | \"How much am I deviating from my prior beliefs?\" |\n",
        "\n",
        "##### The Hessian Structure\n",
        "\n",
        "```math\n",
        "H=X^TSX+\\Sigma_0^{-1}\n",
        "```\n",
        "\n",
        "where `S=diag(̂p ⊙ (1-̂p))` captures the **curvature of the logistic function**.\n",
        "\n",
        "> **Key Insight**: The Hessian combines data curvature (`X^TSX`) with prior curvature (`Σ₀⁻¹`), giving us the complete picture of how the posterior changes.\n",
        "\n",
        "#### Convergence Criteria\n",
        "\n",
        "The algorithm terminates when the step size becomes negligible:\n",
        "\n",
        "```math\n",
        "\\|w_{new} - w_{old}\\| < \\epsilon\n",
        "```\n",
        "\n",
        "This indicates we've found the **peak of the posterior mountain**.\n",
        "\n",
        "## 4.4 Laplace Approximation: Local Gaussian Approximation\n",
        "\n",
        "### The Approximation Philosophy\n",
        "\n",
        "> **The Core Insight**: Even though the true posterior isn't Gaussian, we can approximate it as Gaussian near the MAP estimate. It's like approximating a mountain peak as a smooth hill.\n",
        "\n",
        "#### The Mathematical Approximation\n",
        "\n",
        "We replace the complex posterior with a simple Gaussian:\n",
        "\n",
        "```math\n",
        "q(w)=\\mathcal{N}(\\hat{w}_{MAP},\\,\\Sigma)\n",
        "```\n",
        "\n",
        "where the covariance captures the local curvature:\n",
        "\n",
        "```math\n",
        "\\Sigma=-(\\nabla^2 \\log p(w\\mid D))^{-1}=H^{-1}\n",
        "```\n",
        "\n",
        "#### Why This Approximation Works\n",
        "\n",
        "| **Mathematical Principle** | **Practical Implication** |\n",
        "|---------------------------|---------------------------|\n",
        "| **Taylor Expansion** | Most functions look quadratic near their peaks |\n",
        "| **Central Limit Theorem** | Large samples make posteriors more Gaussian |\n",
        "| **Hessian Information** | Curvature tells us how \"sharp\" the peak is |\n",
        "\n",
        "> **Key Insight**: The Hessian eigenvalues reveal the posterior's \"tightness\"—larger eigenvalues mean the posterior is more concentrated around the mode.\n",
        "\n",
        "#### Making Predictions with Uncertainty\n",
        "\n",
        "Since we can't integrate analytically, we use **Monte Carlo sampling**:\n",
        "\n",
        "```math\n",
        "\\mathbb{E}[\\sigma(w^T x_{new})] \\approx \\frac{1}{S}\\sum_{s=1}^S \\sigma(w_s^T x_{new}),\\; w_s\\sim q(w)\n",
        "```\n",
        "\n",
        "**The Process**:\n",
        "1. **Sample** parameters from the Gaussian approximation\n",
        "2. **Compute** predictions for each sample\n",
        "3. **Average** to get the expected prediction\n",
        "\n",
        "#### Trade-offs and Limitations\n",
        "\n",
        "##### ✅ **Advantages**\n",
        "\n",
        "| **Benefit** | **Why It Matters** |\n",
        "|-------------|-------------------|\n",
        "| **Speed** | Much faster than MCMC |\n",
        "| **Uncertainty** | Captures local uncertainty around mode |\n",
        "| **Simplicity** | Easy to implement and understand |\n",
        "| **Scalability** | Works well with high-dimensional problems |\n",
        "\n",
        "##### ⚠️ **Limitations**\n",
        "\n",
        "| **Limitation** | **When It Fails** |\n",
        "|----------------|-------------------|\n",
        "| **Local Only** | Only captures uncertainty near the mode |\n",
        "| **Unimodal Assumption** | Fails for multimodal posteriors |\n",
        "| **Gaussian Shape** | Assumes posterior is roughly Gaussian |\n",
        "| **Tail Behavior** | May underestimate uncertainty in tails |\n",
        "\n",
        "#### When to Use Laplace Approximation\n",
        "\n",
        "### When to Use Laplace vs MCMC (no diagram)\n",
        "\n",
        "- Unimodal and roughly Gaussian posterior → Use Laplace (fast, local uncertainty)\n",
        "- Multimodal or complex posterior → Use MCMC (slow, full uncertainty)\n",
        "\n",
        "Implications:\n",
        "- Laplace: Fast, good near the mode\n",
        "- MCMC: Full uncertainty, robust to complex shapes\n",
        "\n",
        "## 4.5 Metropolis-Hastings: Sampling the True Posterior\n",
        "\n",
        "### The Sampling Revolution\n",
        "\n",
        "> **The Ultimate Goal**: Generate samples from the true posterior `p(w|D)`, even when we can't write it down analytically. No approximations, no shortcuts—just the real thing.\n",
        "\n",
        "#### The Brilliant Insight\n",
        "\n",
        "We don't need the full posterior! We only need to evaluate it up to a constant:\n",
        "\n",
        "```math\n",
        "g(w)=p(t\\mid X,w)\\,p(w)\n",
        "```\n",
        "\n",
        "> **Why This Matters**: The normalizing constant is often intractable, but we can work with the unnormalized posterior for sampling purposes.\n",
        "\n",
        "#### The Metropolis-Hastings Algorithm\n",
        "\n",
        "##### The Core Mechanism\n",
        "\n",
        "### Metropolis–Hastings Steps (no diagram)\n",
        "\n",
        "1) Start at current `w`\n",
        "2) Propose `w' ~ q(w' | w)` (e.g., Gaussian random walk)\n",
        "3) Compute acceptance ratio `r = g(w') / g(w)` with `g(w) = p(t|X,w)p(w)`\n",
        "4) Accept with probability `min(1, r)`; otherwise stay at `w`\n",
        "5) Repeat to build a Markov chain\n",
        "\n",
        "##### The Acceptance Rule\n",
        "\n",
        "```math\n",
        "\\text{Accept } w' \\text{ with probability } \\min(1,r)\n",
        "```\n",
        "\n",
        "where `r = g(w')/g(w)` is the **acceptance ratio**.\n",
        "\n",
        "> **The Magic**: This simple rule ensures we visit high-probability regions more often, eventually sampling proportionally to the true posterior.\n",
        "\n",
        "#### Proposal Strategies\n",
        "\n",
        "##### Random Walk Proposals\n",
        "\n",
        "The most common approach uses a Gaussian random walk:\n",
        "\n",
        "```math\n",
        "w'\\sim \\mathcal{N}(w,\\Sigma_q)\n",
        "```\n",
        "\n",
        "**Tuning the Proposal**:\n",
        "\n",
        "| **Parameter** | **Effect** | **Guideline** |\n",
        "|---------------|------------|---------------|\n",
        "| **Large `Σ_q`** | More exploration, lower acceptance | Good for multimodal posteriors |\n",
        "| **Small `Σ_q`** | Less exploration, higher acceptance | Good for unimodal posteriors |\n",
        "| **Optimal Size** | ~25% acceptance rate | Balance between exploration and efficiency |\n",
        "\n",
        "#### Practical Considerations\n",
        "\n",
        "##### 🔧 **Tuning Parameters**\n",
        "\n",
        "| **Parameter** | **Purpose** | **Best Practice** |\n",
        "|---------------|-------------|-------------------|\n",
        "| **Step Size** | Controls proposal variance | Tune for 20-40% acceptance rate |\n",
        "| **Burn-in** | Remove initial samples | Discard first 10-50% of samples |\n",
        "| **Multiple Chains** | Check convergence | Run 3-4 chains from different starts |\n",
        "| **Thinning** | Reduce autocorrelation | Keep every k-th sample |\n",
        "\n",
        "##### 📊 **Convergence Diagnostics**\n",
        "\n",
        "**Essential Checks**:\n",
        "\n",
        "| **Diagnostic** | **What It Measures** | **Good Value** |\n",
        "|----------------|---------------------|----------------|\n",
        "| **R-hat** | Between vs within-chain variance | ≈ 1.0 (≤ 1.1) |\n",
        "| **Trace Plots** | Visual chain behavior | No trends, good mixing |\n",
        "| **Effective Sample Size** | Independent samples | > 1000 per parameter |\n",
        "\n",
        "#### Performance Analysis\n",
        "\n",
        "##### ✅ **Advantages**\n",
        "\n",
        "| **Strength** | **Why It Matters** |\n",
        "|--------------|-------------------|\n",
        "| **True Sampling** | No approximations, exact results |\n",
        "| **Multimodal Handling** | Works with complex posterior shapes |\n",
        "| **No Assumptions** | No distributional requirements |\n",
        "| **Flexible** | Works with any unnormalized posterior |\n",
        "\n",
        "##### ⚠️ **Challenges**\n",
        "\n",
        "| **Challenge** | **Impact** | **Mitigation** |\n",
        "|---------------|------------|----------------|\n",
        "| **Computational Cost** | Slow for large problems | Use faster alternatives when possible |\n",
        "| **Tuning Required** | Manual parameter adjustment | Use adaptive algorithms |\n",
        "| **Convergence Issues** | May not converge | Multiple chains, longer runs |\n",
        "| **Autocorrelation** | Samples not independent | Thinning, longer chains |\n",
        "\n",
        "#### When to Choose MCMC (no diagram)\n",
        "\n",
        "**Decision Tree:**\n",
        "1. Need full uncertainty? → Yes\n",
        "2. Posterior complexity?\n",
        "   - Simple & unimodal → Consider Laplace first\n",
        "     - Fast enough? → Use Laplace (local uncertainty only)\n",
        "     - Too slow? → Use MCMC (true posterior sampling)\n",
        "   - Complex or multimodal → Use MCMC (true posterior sampling)\n",
        "\n",
        "**Key Point:** MCMC gives you the true posterior, but Laplace is faster for simple cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "f-string: single '}' is not allowed (679573655.py, line 90)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"   Laplace prediction accuracy: {(probs_laplace > 0.5).astype(int) == y}.mean():.3f}\\n\")\u001b[39m\n                                                                                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m f-string: single '}' is not allowed\n"
          ]
        }
      ],
      "source": [
        "# Chapter 4 Demo: Logistic MAP, Laplace, and MH Sampling\n",
        "# This example demonstrates all three approximation methods on the same logistic regression problem\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import inv\n",
        "from scipy.special import expit\n",
        "from sklearn.metrics import log_loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "print(\"=== Chapter 4: Logistic Regression Approximation Methods ===\\n\")\n",
        "\n",
        "# 1. Generate synthetic binary classification data\n",
        "print(\"1. Generating synthetic data...\")\n",
        "N = 300  # Number of data points\n",
        "X = rng.normal(size=(N, 2))  # 2D features\n",
        "w_true = np.array([1.2, -1.6])  # True parameters\n",
        "logits = X @ w_true  # Linear combination\n",
        "p_true = expit(logits)  # True probabilities\n",
        "y = rng.binomial(1, p_true)  # Binary outcomes\n",
        "\n",
        "# Add bias term (intercept)\n",
        "Xb = np.hstack([np.ones((N,1)), X])\n",
        "D = Xb.shape[1]  # Number of parameters (including bias)\n",
        "\n",
        "print(f\"   Data shape: {Xb.shape}\")\n",
        "print(f\"   True parameters: {w_true}\")\n",
        "print(f\"   Class balance: {y.mean():.3f} positive class\\n\")\n",
        "\n",
        "# 2. Set up prior: w ~ N(0, tau^2 I)\n",
        "tau2 = 5.0  # Prior variance\n",
        "Sigma0_inv = np.eye(D) / tau2  # Prior precision matrix\n",
        "print(f\"2. Prior: w ~ N(0, {tau2}I)\\n\")\n",
        "\n",
        "# 3. MAP estimation via Newton-Raphson\n",
        "print(\"3. MAP Estimation (Newton-Raphson)...\")\n",
        "\n",
        "def logistic_log_posterior(w, X, y, Sigma0_inv):\n",
        "    \"\"\"Compute log posterior for logistic regression\"\"\"\n",
        "    z = X @ w\n",
        "    log_likelihood = (y * z - np.log1p(np.exp(z))).sum()\n",
        "    log_prior = -0.5 * (w @ Sigma0_inv @ w)\n",
        "    return log_likelihood + log_prior\n",
        "\n",
        "# Newton-Raphson optimization\n",
        "w = np.zeros(D)  # Initialize\n",
        "max_iter = 50\n",
        "tolerance = 1e-6\n",
        "\n",
        "for iteration in range(max_iter):\n",
        "    # Current predictions\n",
        "    z = Xb @ w\n",
        "    p_hat = expit(z)\n",
        "    \n",
        "    # Gradient: X^T(p_hat - y) + Sigma0_inv * w\n",
        "    grad = Xb.T @ (p_hat - y) + Sigma0_inv @ w\n",
        "    \n",
        "    # Hessian: X^T * S * X + Sigma0_inv\n",
        "    S = np.diag(p_hat * (1 - p_hat))  # Diagonal matrix\n",
        "    H = Xb.T @ S @ Xb + Sigma0_inv\n",
        "    \n",
        "    # Newton step: w = w - H^(-1) * grad\n",
        "    step = inv(H) @ grad\n",
        "    w -= step\n",
        "    \n",
        "    # Check convergence\n",
        "    if np.linalg.norm(step) < tolerance:\n",
        "        break\n",
        "\n",
        "w_map = w.copy()\n",
        "print(f\"   Converged in {iteration+1} iterations\")\n",
        "print(f\"   MAP estimate: {w_map}\")\n",
        "print(f\"   Log posterior at MAP: {logistic_log_posterior(w_map, Xb, y, Sigma0_inv):.4f}\\n\")\n",
        "\n",
        "# 4. Laplace Approximation\n",
        "print(\"4. Laplace Approximation...\")\n",
        "\n",
        "# Compute Hessian at MAP (already computed above)\n",
        "Sigma_laplace = inv(H)\n",
        "print(f\"   Laplace covariance shape: {Sigma_laplace.shape}\")\n",
        "print(f\"   Laplace std devs: {np.sqrt(np.diag(Sigma_laplace))}\")\n",
        "\n",
        "# Laplace predictive probabilities\n",
        "S_samples = 2000\n",
        "ws_laplace = rng.multivariate_normal(w_map, Sigma_laplace, size=S_samples)\n",
        "probs_laplace = expit(ws_laplace @ Xb.T).mean(axis=0)\n",
        "\n",
        "print(f\"   Laplace prediction accuracy: {(probs_laplace > 0.5).astype(int) == y}.mean():.3f}\\n\")\n",
        "\n",
        "# 5. Metropolis-Hastings Sampling\n",
        "print(\"5. Metropolis-Hastings Sampling...\")\n",
        "\n",
        "def log_posterior_unnormalized(w):\n",
        "    \"\"\"Unnormalized log posterior (up to constant)\"\"\"\n",
        "    return logistic_log_posterior(w, Xb, y, Sigma0_inv)\n",
        "\n",
        "# MH parameters\n",
        "n_samples = 6000\n",
        "burn_in = 1000\n",
        "prop_cov = 0.1 * Sigma_laplace  # Proposal covariance (tuned for ~30% acceptance)\n",
        "\n",
        "# Initialize chain\n",
        "w_current = w_map.copy()\n",
        "logp_current = log_posterior_unnormalized(w_current)\n",
        "\n",
        "# Storage\n",
        "chain = []\n",
        "acceptances = 0\n",
        "\n",
        "print(f\"   Running {n_samples} iterations with {burn_in} burn-in...\")\n",
        "\n",
        "for s in range(n_samples):\n",
        "    # Propose new point\n",
        "    w_proposal = rng.multivariate_normal(w_current, prop_cov)\n",
        "    logp_proposal = log_posterior_unnormalized(w_proposal)\n",
        "    \n",
        "    # Acceptance ratio\n",
        "    log_ratio = logp_proposal - logp_current\n",
        "    \n",
        "    # Accept or reject\n",
        "    if np.log(rng.uniform()) < log_ratio:\n",
        "        w_current = w_proposal\n",
        "        logp_current = logp_proposal\n",
        "        acceptances += 1\n",
        "    \n",
        "    chain.append(w_current.copy())\n",
        "\n",
        "chain = np.array(chain)\n",
        "acceptance_rate = acceptances / n_samples\n",
        "\n",
        "print(f\"   Acceptance rate: {acceptance_rate:.3f}\")\n",
        "print(f\"   Chain shape: {chain.shape}\")\n",
        "\n",
        "# Remove burn-in\n",
        "chain_post_burn = chain[burn_in:]\n",
        "print(f\"   Post-burn-in samples: {len(chain_post_burn)}\")\n",
        "\n",
        "# MH predictive probabilities\n",
        "probs_mh = expit(chain_post_burn @ Xb.T).mean(axis=0)\n",
        "\n",
        "print(f\"   MH prediction accuracy: {(probs_mh > 0.5).astype(int) == y}.mean():.3f}\\n\")\n",
        "\n",
        "# 6. Compare all methods\n",
        "print(\"6. Method Comparison:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# MAP predictions\n",
        "probs_map = expit(Xb @ w_map)\n",
        "\n",
        "# Calculate log-loss (lower is better)\n",
        "logloss_map = log_loss(y, probs_map)\n",
        "logloss_laplace = log_loss(y, probs_laplace)\n",
        "logloss_mh = log_loss(y, probs_mh)\n",
        "\n",
        "print(f\"Log-loss comparison:\")\n",
        "print(f\"  MAP:        {logloss_map:.4f}\")\n",
        "print(f\"  Laplace:    {logloss_laplace:.4f}\")\n",
        "print(f\"  MH:         {logloss_mh:.4f}\")\n",
        "\n",
        "print(f\"\\nPrediction accuracy:\")\n",
        "print(f\"  MAP:        {(probs_map > 0.5).astype(int) == y}.mean():.3f}\")\n",
        "print(f\"  Laplace:    {(probs_laplace > 0.5).astype(int) == y}.mean():.3f}\")\n",
        "print(f\"  MH:         {(probs_mh > 0.5).astype(int) == y}.mean():.3f}\")\n",
        "\n",
        "# 7. Visualize results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Parameter traces (MH)\n",
        "axes[0,0].plot(chain_post_burn[:, 1], alpha=0.7, label='w1')\n",
        "axes[0,0].plot(chain_post_burn[:, 2], alpha=0.7, label='w2')\n",
        "axes[0,0].axhline(w_true[0], color='red', linestyle='--', label='True w1')\n",
        "axes[0,0].axhline(w_true[1], color='orange', linestyle='--', label='True w2')\n",
        "axes[0,0].set_title('MH Parameter Traces')\n",
        "axes[0,0].set_xlabel('Iteration')\n",
        "axes[0,0].set_ylabel('Parameter Value')\n",
        "axes[0,0].legend()\n",
        "\n",
        "# Parameter distributions\n",
        "axes[0,1].hist(chain_post_burn[:, 1], bins=50, alpha=0.7, density=True, label='MH w1')\n",
        "axes[0,1].hist(chain_post_burn[:, 2], bins=50, alpha=0.7, density=True, label='MH w2')\n",
        "axes[0,1].axvline(w_map[1], color='red', linestyle='-', label='MAP w1')\n",
        "axes[0,1].axvline(w_map[2], color='orange', linestyle='-', label='MAP w2')\n",
        "axes[0,1].set_title('Parameter Distributions')\n",
        "axes[0,1].set_xlabel('Parameter Value')\n",
        "axes[0,1].set_ylabel('Density')\n",
        "axes[0,1].legend()\n",
        "\n",
        "# Prediction comparison\n",
        "axes[1,0].scatter(probs_map, probs_laplace, alpha=0.6, s=20)\n",
        "axes[1,0].plot([0,1], [0,1], 'r--', alpha=0.8)\n",
        "axes[1,0].set_xlabel('MAP Predictions')\n",
        "axes[1,0].set_ylabel('Laplace Predictions')\n",
        "axes[1,0].set_title('MAP vs Laplace Predictions')\n",
        "\n",
        "axes[1,1].scatter(probs_map, probs_mh, alpha=0.6, s=20)\n",
        "axes[1,1].plot([0,1], [0,1], 'r--', alpha=0.8)\n",
        "axes[1,1].set_xlabel('MAP Predictions')\n",
        "axes[1,1].set_ylabel('MH Predictions')\n",
        "axes[1,1].set_title('MAP vs MH Predictions')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== Key Takeaways ===\")\n",
        "print(\"• MAP: Fastest, single point estimate, no uncertainty\")\n",
        "print(\"• Laplace: Fast, captures local uncertainty, assumes Gaussian shape\")\n",
        "print(\"• MH: Slowest, captures full uncertainty, handles any posterior shape\")\n",
        "print(\"• All methods give similar predictions when posterior is unimodal\")\n",
        "print(\"• MH provides full parameter uncertainty for credible intervals\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part II: Data Science Pipeline\n",
        "\n",
        "## 3. Data Wrangling: From Raw Data to Analysis-Ready\n",
        "\n",
        "### 🎯 **What is Data Wrangling?**\n",
        "\n",
        "> **The 80/20 Rule of Data Science**: Data scientists spend 80% of their time preparing data and only 20% on actual analysis. This isn't inefficiency—it's necessity.\n",
        "\n",
        "#### The Definition\n",
        "\n",
        "**Data Wrangling** is the systematic process of cleaning, transforming, and preparing raw data for analysis. It's the bridge between messy reality and clean analysis.\n",
        "\n",
        "#### Why This Matters More Than You Think\n",
        "\n",
        "| **Challenge** | **Impact** | **Example** |\n",
        "|---------------|------------|-------------|\n",
        "| **Format Inconsistency** | Analysis fails | `WindSpeed9AM` vs `wind_speed_9am` |\n",
        "| **Missing Values** | Biased results | 30% missing in critical variable |\n",
        "| **Wrong Data Types** | Computation errors | Numbers stored as text |\n",
        "| **Inconsistent Naming** | Code breaks | `Location` vs `location` vs `LOCATION` |\n",
        "\n",
        "> **The Bottom Line**: Garbage in, garbage out. No amount of sophisticated modeling can fix fundamentally flawed data.\n",
        "\n",
        "### 📊 **The Data Wrangling Pipeline**\n",
        "\n",
        "#### **Step 1: Data Ingestion**\n",
        "\n",
        "> **The Foundation**: Before you can analyze anything, you need to get your data into your analysis environment.\n",
        "\n",
        "##### Data Source Landscape\n",
        "\n",
        "| **Source Type** | **Complexity** | **Common Use Cases** |\n",
        "|-----------------|----------------|---------------------|\n",
        "| **CSV Files** | 🟢 Simple | Most common starting point |\n",
        "| **Excel Spreadsheets** | 🟡 Moderate | Business reports, legacy data |\n",
        "| **Database Connections** | 🟠 Complex | Large datasets, real-time data |\n",
        "| **APIs & Web Scraping** | 🔴 Advanced | External data sources |\n",
        "| **JSON/XML Files** | 🟡 Moderate | Structured web data |\n",
        "\n",
        "##### Critical Best Practices\n",
        "\n",
        "**Template Variables**: \n",
        "> Use generic variable names like `ds` instead of `weatherAUS`. This creates reusable code that works with any dataset.\n",
        "\n",
        "**Data Structure**:\n",
        "> **Data Frames** are your primary data structure—tables where rows = observations and columns = variables.\n",
        "\n",
        "**Memory Management**:\n",
        "> ⚠️ **Warning**: Be careful not to overwrite processed datasets accidentally. Always keep backups of your original data.\n",
        "\n",
        "#### **Step 2: Initial Data Review**\n",
        "\n",
        "> **The Detective Work**: Before diving into analysis, you need to understand what you're working with. This is like examining a crime scene before solving the case.\n",
        "\n",
        "##### The Essential Checklist\n",
        "\n",
        "| **Aspect** | **What to Look For** | **Why It Matters** |\n",
        "|------------|---------------------|-------------------|\n",
        "| **Shape** | Rows × Columns | Understanding data size |\n",
        "| **Data Types** | Numeric, character, date, factor | Correct analysis methods |\n",
        "| **Missing Values** | Patterns and amounts | Data quality assessment |\n",
        "| **Sample Values** | First, last, random rows | Data content validation |\n",
        "\n",
        "##### Your Investigation Toolkit\n",
        "\n",
        "**Dimension Functions**:\n",
        "```r\n",
        "dim(ds)      # Overall dimensions\n",
        "nrow(ds)     # Number of observations  \n",
        "ncol(ds)     # Number of variables\n",
        "```\n",
        "\n",
        "**Structure Overview**:\n",
        "```r\n",
        "glimpse(ds)   # Quick structure summary\n",
        "str(ds)      # Detailed structure\n",
        "```\n",
        "\n",
        "**Data Sampling**:\n",
        "```r\n",
        "head(ds)     # First 6 rows\n",
        "tail(ds)     # Last 6 rows\n",
        "sample_n(ds, 10)  # Random 10 rows\n",
        "```\n",
        "\n",
        "> **Pro Tip**: Always start with `glimpse()`—it gives you the most information in the least time.\n",
        "\n",
        "#### **Step 3: Data Cleaning**\n",
        "\n",
        "> **The Transformation**: This is where messy data becomes analysis-ready. Think of it as organizing a chaotic room before you can work in it.\n",
        "\n",
        "##### The Three Pillars of Data Cleaning\n",
        "\n",
        "#### 🏷️ **1. Variable Name Normalization**\n",
        "\n",
        "**The Problem**: Inconsistent naming conventions create chaos.\n",
        "\n",
        "| **Before** | **After** | **Rule Applied** |\n",
        "|------------|-----------|------------------|\n",
        "| `WindSpeed9AM` | `wind_speed_9am` | Snake case |\n",
        "| `V004` | `variable_004` | Descriptive names |\n",
        "| `LOCATION` | `location` | Lowercase |\n",
        "\n",
        "**The Solution**:\n",
        "```r\n",
        "# Normalize all variable names\n",
        "names(ds) <- normVarNames(names(ds))\n",
        "```\n",
        "\n",
        "#### 🔄 **2. Data Type Conversion**\n",
        "\n",
        "**The Challenge**: Data often comes in the wrong format.\n",
        "\n",
        "| **Conversion Type** | **When to Use** | **Example** |\n",
        "|---------------------|-----------------|-------------|\n",
        "| **Character → Factor** | Limited categorical values | `\"Yes\"/\"No\"` → Factor |\n",
        "| **Character → Numeric** | Numbers stored as text | `\"123.45\"` → 123.45 |\n",
        "| **String → Date** | Date strings | `\"2023-01-15\"` → Date object |\n",
        "\n",
        "**Common Patterns**:\n",
        "```r\n",
        "# Convert to factors\n",
        "ds$rain_tomorrow <- as.factor(ds$rain_tomorrow)\n",
        "\n",
        "# Convert to numeric (handles missing values)\n",
        "ds$temperature <- as.numeric(ds$temperature)\n",
        "\n",
        "# Parse dates\n",
        "ds$date <- as.Date(ds$date, format = \"%Y-%m-%d\")\n",
        "```\n",
        "\n",
        "#### 🎯 **3. Factor Level Management**\n",
        "\n",
        "**The Art**: Factors need proper ordering and clean levels.\n",
        "\n",
        "**Ordered Factors** (Natural ordering):\n",
        "```r\n",
        "# Wind directions have natural order\n",
        "wind_levels <- c(\"N\", \"NNE\", \"NE\", \"ENE\", \"E\", \"ESE\", \"SE\", \"SSE\", \n",
        "                 \"S\", \"SSW\", \"SW\", \"WSW\", \"W\", \"WNW\", \"NW\", \"NNW\")\n",
        "ds$wind_dir <- factor(ds$wind_dir, levels = wind_levels, ordered = TRUE)\n",
        "```\n",
        "\n",
        "**Level Normalization**:\n",
        "```r\n",
        "# Remove extra spaces and standardize case\n",
        "ds$location <- trimws(tolower(ds$location))\n",
        "```\n",
        "\n",
        "#### **Step 4: Variable Role Identification**\n",
        "**Purpose**: Categorize variables based on their role in analysis.\n",
        "\n",
        "**Variable Types**:\n",
        "- **Target Variable**: What you want to predict (e.g., rain_tomorrow)\n",
        "- **Risk Variable**: Measures impact/severity of outcome (e.g., amount of rain)\n",
        "- **Identifier Variables**: Unique identifiers (e.g., date, location, ID numbers)\n",
        "- **Input Variables**: Features used to predict the target\n",
        "\n",
        "**Why This Matters**: You need to know which variables to use for modeling and which to exclude.\n",
        "\n",
        "#### **Step 5: Feature Selection**\n",
        "**Purpose**: Remove variables that are inappropriate or unhelpful for modeling.\n",
        "\n",
        "**Variables to Remove**:\n",
        "- **Identifiers**: Unique values for each observation\n",
        "- **Risk Variables**: Output variables that shouldn't be inputs\n",
        "- **All Missing**: Variables with no data\n",
        "- **High Missing**: Variables with >80% missing values\n",
        "- **Too Many Levels**: Factors with excessive categories\n",
        "- **Constants**: Variables with only one value\n",
        "- **Highly Correlated**: Variables that provide redundant information\n",
        "\n",
        "**Correlation Analysis**: \n",
        "- Calculate pairwise correlations between numeric variables\n",
        "- Remove one variable from highly correlated pairs (>0.90)\n",
        "- Use domain knowledge to decide which to keep\n",
        "\n",
        "#### **Step 6: Missing Data Handling**\n",
        "**Purpose**: Deal with missing values appropriately.\n",
        "\n",
        "**Strategies**:\n",
        "1. **Remove Observations**: Delete rows with missing target values\n",
        "2. **Remove Variables**: Delete variables with too many missing values\n",
        "3. **Imputation**: Fill missing values with estimates\n",
        "   - **Mean/Median**: For numeric variables\n",
        "   - **Mode**: For categorical variables\n",
        "   - **Model-based**: Use other variables to predict missing values\n",
        "\n",
        "**Important**: Always investigate why data is missing - it might reveal systematic issues.\n",
        "\n",
        "#### **Step 7: Feature Engineering**\n",
        "**Purpose**: Create new variables that might improve model performance.\n",
        "\n",
        "**Types of New Features**:\n",
        "1. **Derived Features**: \n",
        "   - Extract year, month, season from dates\n",
        "   - Calculate ratios, differences, or combinations\n",
        "   - Create interaction terms\n",
        "\n",
        "2. **Model-Generated Features**:\n",
        "   - **Clustering**: Group similar observations\n",
        "   - **Dimensionality Reduction**: PCA, factor analysis\n",
        "   - **Domain-Specific**: Expert knowledge-based features\n",
        "\n",
        "#### **Step 8: Data Partitioning**\n",
        "**Purpose**: Split data for proper model evaluation.\n",
        "\n",
        "**Standard Split**:\n",
        "- **Training Set (70%)**: Build the model\n",
        "- **Validation Set (15%)**: Tune model parameters\n",
        "- **Test Set (15%)**: Final performance evaluation\n",
        "\n",
        "**Why Three Sets**: Prevents overfitting and provides unbiased performance estimates.\n",
        "\n",
        "#### **Step 9: Metadata Preparation**\n",
        "**Purpose**: Record information about your dataset for future reference.\n",
        "\n",
        "**What to Record**:\n",
        "- Dataset name and source\n",
        "- Number of observations and variables\n",
        "- Variable names and types\n",
        "- Target and input variable lists\n",
        "- Missing value patterns\n",
        "- Data collection dates\n",
        "\n",
        "### 🐍 **Python Equivalent: Pandas Data Wrangling**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Data Ingestion\n",
        "df = pd.read_csv('weather_data.csv')\n",
        "\n",
        "# 2. Initial Review\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Data types:\\n{df.dtypes}\")\n",
        "print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
        "\n",
        "# 3. Data Cleaning\n",
        "# Normalize column names\n",
        "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Convert data types\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['rain_tomorrow'] = df['rain_tomorrow'].astype('category')\n",
        "\n",
        "# 4. Feature Selection\n",
        "# Remove high missing columns\n",
        "high_missing = df.columns[df.isnull().sum() / len(df) > 0.8]\n",
        "df = df.drop(columns=high_missing)\n",
        "\n",
        "# Remove highly correlated variables\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "corr_matrix = df[numeric_cols].corr().abs()\n",
        "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "high_corr = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
        "df = df.drop(columns=high_corr)\n",
        "\n",
        "# 5. Missing Data Handling\n",
        "# Option 1: Remove rows with missing target\n",
        "df = df.dropna(subset=['rain_tomorrow'])\n",
        "\n",
        "# Option 2: Impute missing values\n",
        "df = df.fillna(df.median())  # For numeric\n",
        "df = df.fillna(df.mode().iloc[0])  # For categorical\n",
        "\n",
        "# 6. Feature Engineering\n",
        "df['year'] = df['date'].dt.year\n",
        "df['season'] = df['date'].dt.month.map({12:1, 1:1, 2:1, 3:2, 4:2, 5:2, \n",
        "                                       6:3, 7:3, 8:3, 9:4, 10:4, 11:4})\n",
        "\n",
        "# 7. Data Partitioning\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, temp = train_test_split(df, test_size=0.3, random_state=42)\n",
        "val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Visualization: Making Data Tell Stories\n",
        "\n",
        "### 🎯 **Why Visualization Matters**\n",
        "\n",
        "> **The Visual Revolution**: In a world drowning in data, visualization is your lifeboat. It transforms abstract numbers into compelling stories that drive decisions.\n",
        "\n",
        "#### The Four Pillars of Visual Data\n",
        "\n",
        "| **Purpose** | **What It Does** | **Real-World Impact** |\n",
        "|-------------|------------------|----------------------|\n",
        "| **🔍 Discovery** | Reveals hidden patterns | Identifies trends before competitors |\n",
        "| **💬 Communication** | Shares findings effectively | Convinces stakeholders to act |\n",
        "| **✅ Validation** | Checks assumptions | Prevents costly mistakes |\n",
        "| **🧭 Exploration** | Guides analysis direction | Focuses effort on what matters |\n",
        "\n",
        "> **The 80/20 Rule**: Spend 80% of your time exploring data visually before diving into complex modeling. This isn't procrastination—it's strategy.\n",
        "\n",
        "### 📊 **The Grammar of Graphics (ggplot2 Philosophy)**\n",
        "\n",
        "> **The Universal Language**: Just as grammar provides rules for constructing sentences, the Grammar of Graphics provides rules for constructing visualizations. Master this, and you can create any plot imaginable.\n",
        "\n",
        "#### The Seven Building Blocks\n",
        "\n",
        "| **Component** | **Role** | **Example** |\n",
        "|---------------|----------|-------------|\n",
        "| **📊 Data** | The raw material | Your dataset |\n",
        "| **🎨 Aesthetics** | Variable → visual mapping | `x=price, y=sales, color=region` |\n",
        "| **🔧 Geometries** | Visual elements | Points, bars, lines, polygons |\n",
        "| **📏 Scales** | How aesthetics display | Log scale, color palette |\n",
        "| **🔀 Facets** | Subplot organization | Split by category |\n",
        "| **📐 Coordinates** | Plot space layout | Cartesian, polar, map projections |\n",
        "| **🎭 Themes** | Visual styling | Colors, fonts, backgrounds |\n",
        "\n",
        "#### The Layered Architecture\n",
        "\n",
        "> **Think Like a Chef**: Just as you layer flavors to create a complex dish, you layer visual elements to create a compelling plot.\n",
        "\n",
        "**The Incremental Building Process**:\n",
        "```r\n",
        "# Start with the foundation\n",
        "ggplot(data, aes(x=var1, y=var2)) +  # Base plot with aesthetics\n",
        "\n",
        "# Add visual elements\n",
        "  geom_point() +                     # Scatter points\n",
        "  geom_smooth() +                    # Trend line\n",
        "  \n",
        "# Apply styling\n",
        "  theme_minimal() +                  # Clean theme\n",
        "  labs(title=\"My Analysis\")          # Labels and title\n",
        "```\n",
        "\n",
        "**Why This Approach Works**:\n",
        "- **Modularity**: Each layer has a specific purpose\n",
        "- **Flexibility**: Mix and match components as needed\n",
        "- **Debugging**: Easy to identify which layer causes issues\n",
        "- **Reusability**: Common patterns become templates\n",
        "\n",
        "### 🔍 **Essential Plot Types for Data Exploration**\n",
        "\n",
        "> **Your Visualization Arsenal**: These four plot types will handle 90% of your data exploration needs. Master them, and you'll be equipped for any analysis.\n",
        "\n",
        "#### 📈 **1. Scatter Plots: The Relationship Detective**\n",
        "\n",
        "**The Mission**: Explore relationships between two continuous variables and uncover hidden patterns.\n",
        "\n",
        "##### When to Deploy Scatter Plots\n",
        "\n",
        "| **Use Case** | **What You'll Discover** | **Example** |\n",
        "|---------------|-------------------------|-------------|\n",
        "| **Linear Relationships** | Correlation strength and direction | Sales vs. Marketing spend |\n",
        "| **Non-linear Patterns** | Curves, exponential growth | Population growth over time |\n",
        "| **Outlier Detection** | Unusual data points | Fraudulent transactions |\n",
        "| **Clustering** | Natural groupings | Customer segments |\n",
        "\n",
        "##### The Aesthetic Toolkit\n",
        "\n",
        "**Core Aesthetics**:\n",
        "```r\n",
        "ggplot(data, aes(\n",
        "  x = variable1,           # Horizontal axis\n",
        "  y = variable2,           # Vertical axis\n",
        "  color = category,        # Group by category\n",
        "  size = importance,       # Encode third variable\n",
        "  alpha = 0.7             # Control transparency\n",
        ")) + geom_point()\n",
        "```\n",
        "\n",
        "##### Common Challenges & Solutions\n",
        "\n",
        "| **Problem** | **Symptoms** | **Solution** |\n",
        "|-------------|--------------|--------------|\n",
        "| **Overplotting** | Points overlap, patterns hidden | Use `alpha` or sample data |\n",
        "| **Scale Mismatch** | One variable dominates | Log transformation |\n",
        "| **Too Many Points** | Plot becomes cluttered | Jittering or sampling |\n",
        "\n",
        "> **Pro Tip**: Always start with `alpha=0.7` to handle overplotting gracefully.\n",
        "\n",
        "#### 📊 **2. Bar Charts: The Categorical Storyteller**\n",
        "\n",
        "**The Mission**: Transform categorical data into visual narratives that reveal patterns and comparisons.\n",
        "\n",
        "##### The Three Bar Chart Archetypes\n",
        "\n",
        "| **Type** | **Best For** | **Visual Pattern** |\n",
        "|----------|--------------|-------------------|\n",
        "| **Count Bar Chart** | Frequency distributions | Heights show counts |\n",
        "| **Stacked Bar Chart** | Proportions within groups | Segments show composition |\n",
        "| **Grouped Bar Chart** | Comparisons across groups | Side-by-side comparisons |\n",
        "\n",
        "##### Strategic Applications\n",
        "\n",
        "**When Bar Charts Excel**:\n",
        "- **Distribution Analysis**: Understanding category frequencies\n",
        "- **Group Comparisons**: Comparing across different segments\n",
        "- **Proportion Visualization**: Showing parts of a whole\n",
        "- **Trend Identification**: Spotting patterns over categories\n",
        "\n",
        "##### Design Principles\n",
        "\n",
        "**Visual Hierarchy**:\n",
        "```r\n",
        "ggplot(data, aes(x = reorder(category, count))) +  # Order by frequency\n",
        "  geom_bar(aes(fill = group), position = \"dodge\") +  # Grouped bars\n",
        "  coord_flip() +  # Horizontal for long labels\n",
        "  scale_fill_brewer(palette = \"Set2\")  # Colorblind-friendly colors\n",
        "```\n",
        "\n",
        "**Critical Design Decisions**:\n",
        "\n",
        "| **Element** | **Best Practice** | **Why It Matters** |\n",
        "|-------------|------------------|-------------------|\n",
        "| **Ordering** | Sort by frequency or logical sequence | Easier pattern recognition |\n",
        "| **Color** | Use 3-5 meaningful colors max | Prevents visual confusion |\n",
        "| **Labels** | Rotate long labels, use abbreviations | Maintains readability |\n",
        "| **Spacing** | Consistent bar widths and gaps | Professional appearance |\n",
        "\n",
        "> **Golden Rule**: If you have more than 7 categories, consider grouping or using a different plot type.\n",
        "\n",
        "#### 📦 **3. Box Plots: The Distribution Detective**\n",
        "\n",
        "**The Mission**: Reveal the complete distribution story of continuous variables across different groups.\n",
        "\n",
        "##### The Anatomy of a Box Plot\n",
        "\n",
        "##### Box Plot Components (no diagram)\n",
        "\n",
        "- Median line: 50th percentile\n",
        "- Quartile box: 25th to 75th percentile (IQR)\n",
        "- Whiskers: extend to 1.5 × IQR from box edges\n",
        "- Outliers: points beyond whiskers\n",
        "\n",
        "##### What Each Element Reveals\n",
        "\n",
        "| **Component** | **Statistical Meaning** | **Visual Interpretation** |\n",
        "|---------------|------------------------|---------------------------|\n",
        "| **Median Line** | 50th percentile | Center of distribution |\n",
        "| **Box Edges** | 25th & 75th percentiles | Middle 50% of data |\n",
        "| **Whiskers** | 1.5 × IQR from box | Typical data range |\n",
        "| **Outliers** | Points beyond whiskers | Unusual observations |\n",
        "\n",
        "##### Strategic Use Cases\n",
        "\n",
        "**When Box Plots Shine**:\n",
        "- **Group Comparisons**: Compare distributions across categories\n",
        "- **Outlier Detection**: Identify unusual data points\n",
        "- **Skewness Assessment**: Detect distribution asymmetry\n",
        "- **Assumption Validation**: Check normality and equal variance\n",
        "\n",
        "##### Advanced Box Plot Techniques\n",
        "\n",
        "**Enhanced Visualization**:\n",
        "```r\n",
        "ggplot(data, aes(x = group, y = value, fill = group)) +\n",
        "  geom_boxplot(alpha = 0.7, outlier.alpha = 0.5) +\n",
        "  geom_violin(alpha = 0.3) +  # Add density information\n",
        "  stat_summary(fun = mean, geom = \"point\", \n",
        "               shape = 20, size = 3, color = \"red\")  # Add mean points\n",
        "```\n",
        "\n",
        "**Why This Combination Works**:\n",
        "- **Box Plot**: Shows summary statistics\n",
        "- **Violin Plot**: Shows distribution shape\n",
        "- **Mean Points**: Highlights central tendency\n",
        "\n",
        "> **Pro Tip**: Overlay violin plots on box plots to get both summary statistics and distribution shape in one visualization.\n",
        "\n",
        "#### 🎻 **4. Violin Plots: The Shape Revealer**\n",
        "\n",
        "**The Mission**: Expose the complete distribution shape, revealing patterns that summary statistics hide.\n",
        "\n",
        "##### The Violin Plot Advantage\n",
        "\n",
        "> **Beyond Summary Statistics**: While box plots show you the \"what,\" violin plots show you the \"how\"—the actual shape of your data distribution.\n",
        "\n",
        "##### What Violin Plots Reveal\n",
        "\n",
        "| **Visual Element** | **Statistical Meaning** | **Insight Provided** |\n",
        "|-------------------|------------------------|---------------------|\n",
        "| **Width** | Density at each value | Where most data lives |\n",
        "| **Shape** | Distribution form | Symmetry, skewness, modes |\n",
        "| **Peaks** | High-density regions | Multiple modes or clusters |\n",
        "| **Tails** | Low-density regions | Outlier behavior |\n",
        "\n",
        "##### Strategic Applications\n",
        "\n",
        "**When Violin Plots Excel**:\n",
        "- **Shape Discovery**: Identify bimodal or skewed distributions\n",
        "- **Mode Detection**: Find multiple peaks in data\n",
        "- **Distribution Comparison**: Compare shapes across groups\n",
        "- **Density Analysis**: Understand where data concentrates\n",
        "\n",
        "##### The Power Combination\n",
        "\n",
        "**Violin + Box Plot Fusion**:\n",
        "```r\n",
        "ggplot(data, aes(x = group, y = value)) +\n",
        "  geom_violin(aes(fill = group), alpha = 0.7) +  # Full distribution shape\n",
        "  geom_boxplot(width = 0.1, alpha = 0.8) +      # Summary statistics\n",
        "  scale_fill_brewer(palette = \"Pastel1\")        # Soft colors\n",
        "```\n",
        "\n",
        "**Why This Combination is Powerful**:\n",
        "- **Violin Plot**: Shows complete distribution shape\n",
        "- **Box Plot**: Provides familiar summary statistics\n",
        "- **Together**: Maximum information in minimum space\n",
        "\n",
        "##### When to Choose Violin Plots\n",
        "\n",
        "##### Choosing Box vs Violin (no diagram)\n",
        "\n",
        "- Summary only → Use Box Plot (quick overview)\n",
        "- Full shape needed → Use Violin Plot (detailed analysis)\n",
        "- Both desired → Combine Violin + Box (complete picture)\n",
        "\n",
        "> **Remember**: Violin plots are most valuable when you suspect your data has interesting distribution shapes that summary statistics might miss.\n",
        "\n",
        "### 🎨 **Professional Plot Styling**\n",
        "\n",
        "> **The Art of Visual Communication**: Great data visualization is 50% analysis and 50% design. Master both to create compelling, professional visualizations.\n",
        "\n",
        "#### 🌈 **Color Schemes: The Psychology of Visual Appeal**\n",
        "\n",
        "##### The Four Pillars of Color Design\n",
        "\n",
        "| **Principle** | **Why It Matters** | **Best Practice** |\n",
        "|---------------|-------------------|------------------|\n",
        "| **🎨 Colorblind-Friendly** | 8% of population is colorblind | Use ColorBrewer palettes |\n",
        "| **🧠 Meaningful** | Colors should enhance understanding | Red = danger, green = success |\n",
        "| **🔄 Consistent** | Same colors = same categories | Create color dictionary |\n",
        "| **⚖️ Limited** | Too many colors = confusion | Max 7-8 colors per plot |\n",
        "\n",
        "##### The Color Palette Arsenal\n",
        "\n",
        "**Qualitative Palettes** (Categories):\n",
        "```r\n",
        "# Different colors for different groups\n",
        "scale_color_brewer(palette = \"Set2\")  # Colorblind-friendly\n",
        "scale_color_viridis_d()              # Modern, accessible\n",
        "```\n",
        "\n",
        "**Sequential Palettes** (Ordered Data):\n",
        "```r\n",
        "# Light to dark for continuous data\n",
        "scale_color_viridis_c()              # Perceptually uniform\n",
        "scale_color_brewer(palette = \"Blues\") # Traditional\n",
        "```\n",
        "\n",
        "**Diverging Palettes** (Centered Data):\n",
        "```r\n",
        "# Light in middle, dark at ends\n",
        "scale_color_brewer(palette = \"RdBu\")  # Red-Blue diverging\n",
        "scale_color_gradient2()              # Custom diverging\n",
        "```\n",
        "\n",
        "##### Color Accessibility Checklist\n",
        "\n",
        "✅ **Test Your Colors**:\n",
        "- Use online colorblind simulators\n",
        "- Print in grayscale to check contrast\n",
        "- Ensure sufficient color contrast ratios\n",
        "- Provide alternative encodings (shapes, patterns)\n",
        "\n",
        "#### 📝 **Labels and Titles: The Storytelling Elements**\n",
        "\n",
        "##### The Four Essential Label Types\n",
        "\n",
        "| **Element** | **Purpose** | **Best Practice** |\n",
        "|-------------|-------------|------------------|\n",
        "| **📊 Descriptive Titles** | Answer \"What does this show?\" | Sentence case, specific |\n",
        "| **📏 Axis Labels** | Explain variables and units | Include units, be clear |\n",
        "| **🎨 Legend** | Decode visual encodings | Concise but complete |\n",
        "| **📄 Captions** | Provide context and notes | Additional insights |\n",
        "\n",
        "##### Title Writing Mastery\n",
        "\n",
        "**The Perfect Title Formula**:\n",
        "```\n",
        "[What] + [When/Where] + [Key Finding]\n",
        "```\n",
        "\n",
        "**Examples**:\n",
        "- ❌ **Bad**: \"Sales Data\"\n",
        "- ✅ **Good**: \"Monthly Sales Increased 15% After Marketing Campaign\"\n",
        "- ❌ **Bad**: \"Temperature vs Time\"\n",
        "- ✅ **Good**: \"Average Temperature Trends by Season (2020-2023)\"\n",
        "\n",
        "##### Axis Label Excellence\n",
        "\n",
        "**The Complete Label Checklist**:\n",
        "```r\n",
        "labs(\n",
        "  title = \"Clear, descriptive title\",\n",
        "  subtitle = \"Additional context\",\n",
        "  x = \"Variable Name (Units)\",\n",
        "  y = \"Variable Name (Units)\",\n",
        "  caption = \"Data source and notes\"\n",
        ")\n",
        "```\n",
        "\n",
        "**Common Unit Formats**:\n",
        "- **Currency**: `Revenue ($M)`\n",
        "- **Percentages**: `Growth Rate (%)`\n",
        "- **Time**: `Date`, `Time (hours)`\n",
        "- **Counts**: `Number of Customers`\n",
        "\n",
        "##### Legend Design Principles\n",
        "\n",
        "**When Legends Are Essential**:\n",
        "- Using color to encode categories\n",
        "- Using size to encode continuous variables\n",
        "- Using shape to distinguish groups\n",
        "- Multiple data series\n",
        "\n",
        "**Legend Best Practices**:\n",
        "- **Position**: Right side or bottom (avoid top)\n",
        "- **Clarity**: Use descriptive labels, not variable names\n",
        "- **Consistency**: Same colors across related plots\n",
        "- **Simplicity**: Remove unnecessary legend entries\n",
        "\n",
        "#### 📏 **Scales and Formatting: The Precision Touch**\n",
        "\n",
        "##### Numeric Scale Mastery\n",
        "\n",
        "**The Scale Selection Matrix**:\n",
        "\n",
        "| **Data Type** | **Scale Type** | **When to Use** | **Example** |\n",
        "|---------------|----------------|-----------------|-------------|\n",
        "| **Large Numbers** | Comma separators | Counts, populations | `1,234,567` |\n",
        "| **Wide Range** | Log scale | Orders of magnitude | `10^1` to `10^6` |\n",
        "| **Proportions** | Percentage | Ratios, rates | `25%` instead of `0.25` |\n",
        "| **Money** | Currency | Financial data | `$1.2M`, `€500K` |\n",
        "\n",
        "**Implementation Examples**:\n",
        "```r\n",
        "# Comma separators\n",
        "scale_y_continuous(labels = comma)\n",
        "\n",
        "# Log scale\n",
        "scale_y_log10()\n",
        "\n",
        "# Percentages\n",
        "scale_y_continuous(labels = percent)\n",
        "\n",
        "# Currency\n",
        "scale_y_continuous(labels = dollar_format(scale = 1e-6, suffix = \"M\"))\n",
        "```\n",
        "\n",
        "##### Date Scale Excellence\n",
        "\n",
        "**The Date Formatting Toolkit**:\n",
        "\n",
        "| **Data Frequency** | **Recommended Format** | **Example** |\n",
        "|-------------------|-------------------------|-------------|\n",
        "| **Daily** | Month-Day | `Jan 15`, `Mar 22` |\n",
        "| **Weekly** | Month-Year | `Jan 2023`, `Mar 2023` |\n",
        "| **Monthly** | Year-Month | `2023-01`, `2023-03` |\n",
        "| **Yearly** | Year only | `2020`, `2021`, `2022` |\n",
        "\n",
        "**Advanced Date Formatting**:\n",
        "```r\n",
        "# Custom date labels\n",
        "scale_x_date(\n",
        "  labels = date_format(\"%b %Y\"),\n",
        "  breaks = date_breaks(\"3 months\")\n",
        ")\n",
        "\n",
        "# Rotate labels to prevent overlap\n",
        "theme(axis.text.x = element_text(angle = 45, hjust = 1))\n",
        "```\n",
        "\n",
        "##### Scale Design Principles\n",
        "\n",
        "**The Golden Rules**:\n",
        "1. **Start at Zero**: For bar charts and most comparisons\n",
        "2. **Use Log Scale**: When data spans multiple orders of magnitude\n",
        "3. **Consistent Intervals**: Use round numbers (1, 2, 5, 10)\n",
        "4. **Clear Labels**: Avoid overlapping or cramped text\n",
        "5. **Appropriate Precision**: Don't show unnecessary decimal places\n",
        "\n",
        "> **Pro Tip**: Always test your scales by printing your plot—what looks good on screen might be unreadable on paper.\n",
        "\n",
        "### 🔧 **Advanced Visualization Techniques**\n",
        "\n",
        "#### **Faceting**\n",
        "**Purpose**: Create multiple subplots to explore relationships across groups.\n",
        "\n",
        "**Types**:\n",
        "- **Facet Wrap**: Arrange subplots in a grid\n",
        "- **Facet Grid**: Control both rows and columns\n",
        "\n",
        "**When to Use**:\n",
        "- Explore relationships within subgroups\n",
        "- Compare patterns across categories\n",
        "- Avoid overcrowded single plots\n",
        "\n",
        "#### **Interactive Elements**\n",
        "**Modern Tools**:\n",
        "- **Plotly**: Interactive web-based plots\n",
        "- **Shiny**: Interactive dashboards\n",
        "- **Ggplotly**: Convert ggplot2 to interactive\n",
        "\n",
        "**Benefits**:\n",
        "- Hover for details\n",
        "- Zoom and pan\n",
        "- Filter data dynamically\n",
        "- Export high-quality images\n",
        "\n",
        "### 🐍 **Python Equivalent: Seaborn & Matplotlib**\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# 1. Scatter Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(data=df, x='min_temp', y='max_temp', \n",
        "                hue='rain_tomorrow', alpha=0.7)\n",
        "plt.title('Temperature Relationship by Rain Tomorrow')\n",
        "plt.xlabel('Minimum Temperature (°C)')\n",
        "plt.ylabel('Maximum Temperature (°C)')\n",
        "plt.show()\n",
        "\n",
        "# 2. Bar Chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=df, x='wind_dir_3pm', hue='rain_tomorrow')\n",
        "plt.title('Wind Direction Distribution by Rain Tomorrow')\n",
        "plt.xlabel('Wind Direction at 3PM')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# 3. Box Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=df, x='year', y='max_temp', hue='rain_tomorrow')\n",
        "plt.title('Maximum Temperature Distribution by Year and Rain')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Maximum Temperature (°C)')\n",
        "plt.show()\n",
        "\n",
        "# 4. Violin Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(data=df, x='season', y='max_temp', hue='rain_tomorrow')\n",
        "plt.title('Temperature Distribution by Season')\n",
        "plt.xlabel('Season')\n",
        "plt.ylabel('Maximum Temperature (°C)')\n",
        "plt.show()\n",
        "\n",
        "# 5. Correlation Heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "corr_matrix = df[numeric_cols].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Matrix of Numeric Variables')\n",
        "plt.show()\n",
        "\n",
        "# 6. Faceted Plot\n",
        "g = sns.FacetGrid(df, col='season', hue='rain_tomorrow', height=4)\n",
        "g.map(plt.scatter, 'min_temp', 'max_temp', alpha=0.7)\n",
        "g.add_legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 📈 **Visualization Best Practices**\n",
        "\n",
        "#### **Before You Start**\n",
        "1. **Know Your Audience**: Technical vs. business stakeholders\n",
        "2. **Define Your Goal**: Exploration, communication, or validation\n",
        "3. **Choose the Right Plot**: Match plot type to your data and question\n",
        "4. **Consider Data Size**: Sample large datasets for initial exploration\n",
        "\n",
        "#### **During Creation**\n",
        "1. **Start Simple**: Begin with basic plots, add complexity gradually\n",
        "2. **Iterate**: Build plots incrementally, test each addition\n",
        "3. **Check Assumptions**: Verify your plot shows what you think it shows\n",
        "4. **Document**: Add comments explaining your choices\n",
        "\n",
        "#### **After Creation**\n",
        "1. **Validate**: Does the plot answer your question?\n",
        "2. **Refine**: Adjust colors, labels, scales as needed\n",
        "3. **Save**: Use appropriate format and resolution\n",
        "4. **Share**: Include context and interpretation\n",
        "\n",
        "### 🚨 **Common Visualization Mistakes**\n",
        "\n",
        "1. **Misleading Scales**: Starting y-axis at non-zero, using different scales\n",
        "2. **Too Many Elements**: Overcrowded plots with too much information\n",
        "3. **Poor Color Choices**: Colors that are hard to distinguish or interpret\n",
        "4. **Missing Context**: No titles, labels, or explanations\n",
        "5. **Wrong Plot Type**: Using bar charts for continuous data, line charts for categorical\n",
        "6. **Ignoring Outliers**: Not investigating unusual points\n",
        "7. **Overplotting**: Too many points obscuring patterns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part III: Integration & Practice\n",
        "\n",
        "## 🎯 COMPREHENSIVE SUMMARY — Learn What Actually Matters\n",
        "\n",
        "> **Read this like a map.** It connects the theory (Bayesian thinking), the craft (wrangling/visualization), and the reality (approximations) into a single, practical workflow.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Part I — Bayesian Foundations: A Better Way to Think\n",
        "\n",
        "### 🎲 The Coin Game: Uncertainty as a First-Class Citizen\n",
        "\n",
        "**Why it matters**\n",
        "Bayesian thinking is not about “what is the true r,” but “what should I believe about r given what I saw and what I believed before?” It’s how real decisions are made.\n",
        "\n",
        "```math\n",
        "\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}} \n",
        "\\quad\\Rightarrow\\quad\n",
        "P(r\\mid \\text{data}) = \\frac{P(\\text{data}\\mid r)\\,P(r)}{P(\\text{data})}\n",
        "```\n",
        "\n",
        "**Beta–Binomial elegance**\n",
        "- Prior: `r ~ Beta(α, β)`\n",
        "- Data: `y` heads in `N` tosses  \n",
        "- Posterior: `r | data ~ Beta(α+y, β+N−y)`\n",
        "\n",
        "> You literally “add evidence” to prior experience. This gives calibrated uncertainty and sanity checks in small data.\n",
        "\n",
        "### 📊 Advanced Inference You’ll Actually Use\n",
        "\n",
        "- **Evidence (marginal likelihood)** compares models and hyperparameters. It encodes Occam’s Razor: simple models win unless complexity is warranted.\n",
        "- **Bayesian Linear Regression** delivers a predictive distribution, not a line. You get mean ± credible intervals that widen where data is scarce.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ Part II — Approximation & Sampling: When Closed Forms Break\n",
        "\n",
        "Real models are rarely conjugate. You need three tools and when to pick each:\n",
        "\n",
        "| Method | What you get | When to use | Cost |\n",
        "|---|---|---|---|\n",
        "| MAP | Single best parameters | Prototyping, large data | ⚡ Fast |\n",
        "| Laplace | Gaussian around MAP | Local uncertainty is enough | ⚡ Fast |\n",
        "| MCMC (MH) | Samples from true posterior | Full uncertainty required | 🐢 Slower |\n",
        "\n",
        "Key formulas you saw (logistic case):\n",
        "```math\n",
        "\\hat{w}_{MAP}=\\arg\\max_w \\{\\log p(t\\mid X,w)+\\log p(w)\\}\\n\n",
        "w \\leftarrow w - H^{-1}\\nabla,\\quad H=X^TSX+\\Sigma_0^{-1},\\; \\nabla=X^T(\\hat p-t)+\\Sigma_0^{-1}(w-\\mu_0)\n",
        "```\n",
        "```math\n",
        "q(w)=\\mathcal{N}(\\hat{w}_{MAP},\\,H^{-1}) \\quad\\text{(Laplace)}\n",
        "```\n",
        "```math\n",
        "\\text{MH accept: }\\; \\alpha=\\min\\Big(1,\\frac{g(w')}{g(w)}\\Big),\\; g(w)=p(t\\mid X,w)\\,p(w)\n",
        "```\n",
        "\n",
        "> Rule of thumb: If your posterior is unimodal and roughly symmetric, Laplace is a great workhorse. If you need honest uncertainty in tails or there may be multiple modes, sample with MCMC.\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ Part III — Data Science Pipeline: Turn Data Into Signals\n",
        "\n",
        "### Data Wrangling: The 80/20 Reality\n",
        "Good models come from good tables. The pipeline is repeatable:\n",
        "\n",
        "1) Ingest → 2) Review → 3) Clean → 4) Roles → 5) Select → 6) Missing → 7) Engineer → 8) Split → 9) Document\n",
        "\n",
        "Core decisions you’ll make\n",
        "- Normalize names; enforce types (dates, factors).  \n",
        "- Identify roles: target, risk (output-like), ids, inputs.  \n",
        "- Remove constants, id-like, too-missing, too-many-levels, high-correlation twins.  \n",
        "- Partition data before modeling; keep metadata for reproducibility.\n",
        "\n",
        "### Data Visualization: See Before You Model\n",
        "A plot is a sentence built with the Grammar of Graphics: data + aesthetics + geoms + scales + facets + coordinates + theme.\n",
        "\n",
        "Use the right “sentence”:\n",
        "- Scatter: relationships, clusters, outliers  \n",
        "- Bar: categorical counts/proportions, grouped comparisons  \n",
        "- Box + Violin: distribution summaries + shapes  \n",
        "- Facets: small multiples to compare groups\n",
        "\n",
        "Style like a pro\n",
        "- Colorblind-safe palettes; consistent color mapping  \n",
        "- Titles that say what-happened-where-when  \n",
        "- Axes with units; scales with commas/percents/log where needed\n",
        "\n",
        "---\n",
        "\n",
        "## 🎓 Key Takeaways — What To Remember Under Pressure\n",
        "\n",
        "- Bayesian = beliefs updated by evidence. Use priors to encode knowledge and get calibrated uncertainty.\n",
        "- Evidence compares models; don’t overfit priors or architectures.\n",
        "- MAP is a point; Laplace is a local cloud; MCMC is the full landscape.\n",
        "- Data wrangling discipline beats clever modeling on dirty data.\n",
        "- Visualize first. If a plot can’t convince you, it won’t convince others.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Apply Immediately — Minimal Playbook\n",
        "\n",
        "- Logistic regression with MAP → fast baseline  \n",
        "- Laplace around MAP → uncertainty bands in minutes  \n",
        "- MH sampling (thinned, post burn‑in) → credible intervals and calibration\n",
        "- Pandas/Seaborn pipeline → wrangle, split, plot; then model\n",
        "\n",
        "> If you can do the four lines above on any dataset in an hour, you’re operating like a pro.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
