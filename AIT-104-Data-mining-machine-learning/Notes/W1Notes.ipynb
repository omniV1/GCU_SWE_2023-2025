{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e383bf25-4afd-4283-ae1a-a77b88f50b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# Statistical and visualization libraries\n",
    "import statsmodels.api as sm  \n",
    "import seaborn as sns\n",
    "\n",
    "# For reproducibility\n",
    "from pandas.core.common import random_state  # Note: This import might be deprecated\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.linear_model import LinearRegression  # Fixed capitalization\n",
    "from sklearn.model_selection import train_test_split  # Fixed typo: 'sklern' -> 'sklearn'\n",
    "\n",
    "# Set the default seaborn style for plots\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b693b7b-d953-4883-a5bc-ca0d821d88a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close/Last</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09/02/2025</td>\n",
       "      <td>$157.09</td>\n",
       "      <td>65434970</td>\n",
       "      <td>$151.20</td>\n",
       "      <td>$158.39</td>\n",
       "      <td>$150.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08/29/2025</td>\n",
       "      <td>$156.71</td>\n",
       "      <td>45270500</td>\n",
       "      <td>$156.98</td>\n",
       "      <td>$158.42</td>\n",
       "      <td>$153.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08/28/2025</td>\n",
       "      <td>$158.12</td>\n",
       "      <td>57885240</td>\n",
       "      <td>$157.63</td>\n",
       "      <td>$158.23</td>\n",
       "      <td>$152.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08/27/2025</td>\n",
       "      <td>$156.72</td>\n",
       "      <td>76380550</td>\n",
       "      <td>$162.32</td>\n",
       "      <td>$162.40</td>\n",
       "      <td>$155.9801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08/26/2025</td>\n",
       "      <td>$160.87</td>\n",
       "      <td>86573720</td>\n",
       "      <td>$155.39</td>\n",
       "      <td>$162.13</td>\n",
       "      <td>$154.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Close/Last    Volume     Open     High        Low\n",
       "0  09/02/2025    $157.09  65434970  $151.20  $158.39    $150.28\n",
       "1  08/29/2025    $156.71  45270500  $156.98  $158.42    $153.00\n",
       "2  08/28/2025    $158.12  57885240  $157.63  $158.23    $152.55\n",
       "3  08/27/2025    $156.72  76380550  $162.32  $162.40  $155.9801\n",
       "4  08/26/2025    $160.87  86573720  $155.39  $162.13    $154.57"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from local storage\n",
    "data = pd.read_csv('PLTR.csv')\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545fd7f4-ca79-46fe-9fb7-24981d6a01db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the average set up in Jupyter notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9189ee-1b8c-4db7-8779-b10d3e7aa666",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "- is learning (training) and infering (testing) a set of attributes (features,independent variables) and targets (labels, dependent variables)\n",
    "\n",
    "- y = f(x) = ax + b machines can learn a function like this easily\n",
    "\n",
    "\n",
    "# Linear Model\n",
    "1. Practical and most straightforward model\n",
    "2. Linear relations between variables, meaning that, no variable-variable multiplication.\n",
    "    a. Addition of variables\n",
    "    b. Multiplication of a variable and a constant\n",
    "3. Non-Example:\n",
    "   y = ax^2 + bx + c any exponent other than 1 is not LINEAR\n",
    "\n",
    "# Finding the best method \n",
    "\n",
    " - when the line passes through as close as possible to the points\n",
    " \n",
    "    How do we evaluate the concept?\n",
    "\n",
    "   Mean Squared Loss:\n",
    "\n",
    "       find the outer edge vectors, mean and square them to reduce the residual losses. \n",
    "\n",
    "\n",
    "   Mean Squared Loss produces:\n",
    "   \n",
    "         - saddle points - to the left = lower to the right = higher\n",
    "\n",
    "         - local maximum - to the left lower and to the right lower\n",
    "\n",
    "         - local minimum - to the left higher to the right higher\n",
    "\n",
    "The relationship between this and derivatives are: to see which of those points = the relevant data we need. \n",
    "\n",
    "1st and 2nd derivatives are necessary to derive each point! (saddle, max, min) \n",
    "\n",
    "end of 09/03/2025 class notes\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d7cebb-094d-44a0-b5f5-7a52464a8590",
   "metadata": {},
   "source": [
    "# Personal notes on the book [A First Course in Machine Learning Chapter 1](https://bibliu.com/app/#/view/books/9781498738569/epub/OPS/xhtml/12_Chapter01.html#page_18)\n",
    "\n",
    "\n",
    "# Chapter 1: Linear Modelling - A Least Squares Approach\n",
    "\n",
    "## Overview\n",
    "\n",
    "Linear modeling represents one of the most fundamental approaches in machine learning for discovering functional relationships between input attributes and target responses. The chapter demonstrates these concepts through a compelling example: predicting Olympic 100-meter sprint winning times based on the year of competition. This practical application illustrates how machine learning can extract meaningful patterns from historical data to make future predictions.\n",
    "\n",
    "The core premise involves learning a mathematical function that maps input variables (like Olympic year) to output variables (like winning time). Once this relationship is established, the model can predict outcomes for previously unseen inputs. For instance, using Olympic data from 1896 to 2008, we can attempt to predict winning times for the 2012 and 2016 Olympics.\n",
    "\n",
    "## The Linear Model Framework\n",
    "\n",
    "Linear modeling assumes that the relationship between inputs and outputs can be adequately represented by a straight line. In its simplest form, this relationship follows the familiar equation t = w₀ + w₁x, where t represents the target value (winning time), x represents the input attribute (Olympic year), and w₀ and w₁ are parameters that define the line's characteristics.\n",
    "\n",
    "Consider the Olympic example: the data shows that winning times generally decrease over the years, suggesting a negative linear relationship. The parameter w₁ (gradient) captures this downward trend, while w₀ (intercept) represents the theoretical winning time at year zero. Through careful analysis of the Olympic data, the chapter derives the optimal linear model as f(x; w₀, w₁) = 36.416 - 0.013x, indicating that winning times decrease by approximately 0.013 seconds per year on average.\n",
    "\n",
    "| Parameter | Value | Interpretation |\n",
    "|-----------|-------|----------------|\n",
    "| w₀ (intercept) | 36.416 | Theoretical winning time at year 0 |\n",
    "| w₁ (gradient) | -0.013 | Average decrease in seconds per year |\n",
    "\n",
    "The model makes specific predictions: for 2012, it predicts a winning time of 9.595 seconds, and for 2016, 9.541 seconds. These precise predictions highlight both the power and limitations of linear modeling, as real-world events rarely conform to such mathematical precision.\n",
    "\n",
    "## Loss Functions and Optimization\n",
    "\n",
    "The quality of any model depends on how well it fits the observed data. The squared loss function provides an objective measure of model performance by calculating the squared difference between actual and predicted values for each data point. For the nth Olympic year, this loss is expressed as Lₙ = (tₙ - f(xₙ; w₀, w₁))², where tₙ is the actual winning time and f(xₙ; w₀, w₁) is the predicted time.\n",
    "\n",
    "Squaring the differences serves multiple purposes: it prevents positive and negative errors from canceling each other out, it penalizes larger errors more heavily than smaller ones, and it creates a mathematically tractable optimization problem. The total model performance is measured by averaging these individual losses across all data points: L = (1/N) Σ Lₙ.\n",
    "\n",
    "The least squares method finds the parameter values that minimize this average loss. This optimization process involves taking partial derivatives of the loss function with respect to each parameter, setting these derivatives to zero, and solving the resulting system of equations. For the Olympic data, this mathematical procedure yields the optimal parameters mentioned above.\n",
    "\n",
    "## Worked Example with Synthetic Data\n",
    "\n",
    "To illustrate the mechanics of least squares fitting, consider a simple synthetic dataset with three data points:\n",
    "\n",
    "| n | xₙ | tₙ | xₙtₙ | xₙ² |\n",
    "|---|----|----|------|-----|\n",
    "| 1 | 1  | 4.8| 4.8  | 1   |\n",
    "| 2 | 3  |11.3| 33.9 | 9   |\n",
    "| 3 | 5  |17.2| 86   | 25  |\n",
    "|Avg| 3  |11.1| 41.57|11.67|\n",
    "\n",
    "Using the derived formulas, we first calculate w₁ = (41.57 - 3 × 11.1)/(11.67 - 3 × 3) = 8.27/2.67 = 3.1. Then w₀ = 11.1 - 3.1 × 3 = 1.8. The resulting model f(x; w₀, w₁) = 1.8 + 3.1x demonstrates how the least squares method produces a line that balances proximity to all data points rather than passing through any specific point perfectly.\n",
    "\n",
    "## Vector and Matrix Formulation\n",
    "\n",
    "As models become more complex, involving multiple input attributes or higher-order terms, the scalar approach becomes unwieldy. Vector and matrix notation provides an elegant solution that scales to arbitrary complexity. Instead of treating parameters individually, we combine them into a parameter vector w = [w₀, w₁]ᵀ and augment each input with a constant term: xₙ = [1, xₙ]ᵀ.\n",
    "\n",
    "This reformulation transforms the model into f(xₙ; w) = wᵀxₙ, a simple dot product. The loss function becomes L = (1/N)(t - Xw)ᵀ(t - Xw), where X is a matrix containing all input vectors and t is a vector of all target values. This matrix formulation leads to the general solution ŵ = (XᵀX)⁻¹Xᵀt, which applies regardless of the number of parameters or complexity of the input transformations.\n",
    "\n",
    "The power of this approach becomes apparent when extending to multiple attributes. Consider predicting Olympic winning times using both the year and the personal best times of the eight lane competitors. The input vector becomes x = [1, year, s₁, s₂, ..., s₈]ᵀ, creating a 10-dimensional parameter space. The matrix formulation handles this complexity seamlessly, requiring no changes to the solution methodology.\n",
    "\n",
    "## Non-Linear Extensions Through Basis Functions\n",
    "\n",
    "Linear models need not be limited to straight-line relationships. By transforming the input variables through basis functions, we can capture non-linear patterns while maintaining the computational advantages of linear parameter estimation. The most common transformation involves polynomial terms: instead of using just x, we include powers like x², x³, etc.\n",
    "\n",
    "For example, a quadratic model uses the augmented input vector [1, x, x²]ᵀ, resulting in f(x; w) = w₀ + w₁x + w₂x². This model can capture curved relationships in the data while still using the same least squares solution framework. The chapter demonstrates this with an eighth-order polynomial fitted to the Olympic data, showing how higher-order models can achieve lower training errors.\n",
    "\n",
    "The concept extends beyond polynomials to any set of basis functions. For the Olympic data, which exhibits some periodic behavior, a model incorporating trigonometric functions might be appropriate: f(x; w) = w₀ + w₁x + w₂sin((x-a)/b). This approach allows complex, non-linear relationships to be captured within the linear modeling framework.\n",
    "\n",
    "## The Challenge of Overfitting\n",
    "\n",
    "A critical insight emerges when comparing models of different complexity: more complex models invariably fit the training data better, but this improved fit doesn't necessarily translate to better predictions. The eighth-order polynomial fitted to the Olympic data achieves a training loss of 0.459 compared to 1.358 for the linear model, yet its predictions appear unrealistic, especially when extrapolated beyond the observed data range.\n",
    "\n",
    "This phenomenon, known as overfitting, occurs when a model becomes so complex that it memorizes the specific details of the training data rather than learning the underlying pattern. The model effectively becomes overspecialized to the training examples and loses its ability to generalize to new, unseen data. Understanding this trade-off between fitting the training data and maintaining predictive capability represents one of the most important concepts in machine learning.\n",
    "\n",
    "## Model Selection Through Validation\n",
    "\n",
    "To address the overfitting problem, the chapter introduces validation techniques that estimate how well a model will perform on unseen data. The simplest approach involves setting aside a portion of the available data (the validation set) and using it to test models trained on the remaining data (the training set). For the Olympic example, data from 1980 onwards serves as validation data, with earlier years used for training.\n",
    "\n",
    "This validation approach reveals that simpler models often perform better on unseen data despite achieving higher training errors. When polynomial models of increasing order are compared, the training error decreases monotonically with model complexity, but validation error typically decreases initially then increases, creating a U-shaped curve that identifies the optimal model complexity.\n",
    "\n",
    "Cross-validation extends this concept by systematically rotating which data serves as the validation set. In K-fold cross-validation, the data is divided into K equal blocks, with each block serving as validation data while the model is trained on the remaining K-1 blocks. Leave-one-out cross-validation represents the extreme case where each individual data point serves as a validation set. This technique provides a more robust estimate of model performance, especially when data is limited.\n",
    "\n",
    "## Regularization as Complexity Control\n",
    "\n",
    "An alternative approach to preventing overfitting involves explicitly penalizing model complexity during the optimization process. Regularization achieves this by adding a penalty term to the loss function that grows with the magnitude of the model parameters. The most common form, known as Ridge regression or L2 regularization, modifies the loss function to L' = L + λwᵀw.\n",
    "\n",
    "The regularization parameter λ controls the trade-off between fitting the training data accurately and maintaining model simplicity. When λ = 0, we recover the standard least squares solution. As λ increases, the optimization process increasingly favors simpler models with smaller parameter values, even at the cost of slightly higher training error.\n",
    "\n",
    "The regularized solution becomes ŵ = (XᵀX + NλI)⁻¹Xᵀt, where I represents the identity matrix. This modification has the effect of \"shrinking\" parameter estimates toward zero, producing smoother, more generalizable models. The challenge lies in selecting an appropriate value for λ, which again relies on validation techniques to balance complexity and performance.\n",
    "\n",
    "## Practical Implications and Limitations\n",
    "\n",
    "The Olympic sprint example illustrates both the strengths and limitations of linear modeling. The linear trend in the data makes intuitive sense and provides reasonable short-term predictions. However, extrapolating far into the future reveals the model's limitations: it eventually predicts impossible negative winning times and fails to account for physical limits on human performance.\n",
    "\n",
    "These limitations highlight important considerations when applying linear models. First, the linear assumption may be inappropriate for many real-world relationships. Second, predictions become increasingly unreliable as they move further from the training data range. Third, the model's simplicity, while computationally advantageous, may miss important patterns or relationships in the data.\n",
    "\n",
    "Despite these limitations, linear modeling provides an essential foundation for machine learning. Its analytical solutions offer computational efficiency, its mathematical properties are well understood, and its interpretable parameters provide insight into the relationships being modeled. Many advanced machine learning techniques build upon these linear foundations, making mastery of these concepts crucial for understanding more complex methods.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This chapter establishes linear modeling as both a practical tool for regression problems and a theoretical foundation for machine learning. The progression from simple line fitting through matrix formulations to regularization and validation provides a comprehensive introduction to key machine learning concepts. The Olympic sprint example effectively demonstrates how these abstract mathematical concepts apply to real-world prediction problems, while also illustrating the importance of understanding model limitations and the dangers of overfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708883c8-5d49-408c-9227-181ad9e66f2f",
   "metadata": {},
   "source": [
    "---\n",
    "# Chapter 2: Linear Modelling Summary: A Least Squares Approach \n",
    "\n",
    "## Introduction to Linear Modeling\n",
    "\n",
    "Linear modeling forms the cornerstone of machine learning approaches for discovering functional relationships between input attributes and target responses. The chapter uses Olympic 100-meter sprint winning times as a compelling example to demonstrate how machine learning can extract meaningful patterns from historical data and make future predictions. The fundamental premise involves learning a mathematical function that maps input variables to output variables, enabling predictions for previously unseen inputs.\n",
    "\n",
    "The Olympic example illustrates this concept by using historical data from 1896 to 2008 to predict winning times for the 2012 and 2016 Olympics. This practical application demonstrates both the power and limitations of linear modeling approaches in real-world scenarios.\n",
    "\n",
    "## The Linear Model Framework\n",
    "\n",
    "Linear modeling operates under the assumption that the relationship between inputs and outputs can be adequately represented by a straight line. The simplest form follows the equation t = w₀ + w₁x, where t represents the target value, x represents the input attribute, and w₀ and w₁ are parameters defining the line's characteristics.\n",
    "\n",
    "The Olympic data analysis reveals a negative linear relationship, with winning times generally decreasing over the years. Through mathematical analysis, the optimal linear model emerges as f(x; w₀, w₁) = 36.416 - 0.013x, indicating that winning times decrease by approximately 0.013 seconds per year on average.\n",
    "\n",
    "| Parameter | Value | Interpretation |\n",
    "|-----------|-------|----------------|\n",
    "| w₀ (intercept) | 36.416 | Theoretical winning time at year 0 |\n",
    "| w₁ (gradient) | -0.013 | Average decrease in seconds per year |\n",
    "\n",
    "The model generates specific predictions: 9.595 seconds for 2012 and 9.541 seconds for 2016. These precise predictions highlight both the mathematical precision possible with linear modeling and its inherent limitations when applied to real-world events.\n",
    "\n",
    "## Loss Functions and Optimization Principles\n",
    "\n",
    "Model quality depends fundamentally on how well it fits observed data. The squared loss function provides an objective measure by calculating the squared difference between actual and predicted values for each data point. For the nth Olympic year, this loss is expressed as Lₙ = (tₙ - f(xₙ; w₀, w₁))².\n",
    "\n",
    "Squaring the differences serves multiple critical purposes: preventing positive and negative errors from canceling each other, penalizing larger errors more heavily than smaller ones, and creating a mathematically tractable optimization problem. Total model performance is measured by averaging individual losses across all data points: L = (1/N) Σ Lₙ.\n",
    "\n",
    "The least squares method identifies parameter values that minimize this average loss through calculus-based optimization. This process involves taking partial derivatives of the loss function with respect to each parameter, setting these derivatives to zero, and solving the resulting system of equations to find optimal parameters.\n",
    "\n",
    "## Worked Example with Synthetic Data\n",
    "\n",
    "A simple synthetic dataset with three data points illustrates the mechanics of least squares fitting effectively:\n",
    "\n",
    "| n | xₙ | tₙ | xₙtₙ | xₙ² |\n",
    "|---|----|----|------|-----|\n",
    "| 1 | 1  | 4.8| 4.8  | 1   |\n",
    "| 2 | 3  |11.3| 33.9 | 9   |\n",
    "| 3 | 5  |17.2| 86   | 25  |\n",
    "|Avg| 3  |11.1| 41.57|11.67|\n",
    "\n",
    "Using derived formulas, w₁ = (41.57 - 3 × 11.1)/(11.67 - 3 × 3) = 8.27/2.67 = 3.1, and w₀ = 11.1 - 3.1 × 3 = 1.8. The resulting model f(x; w₀, w₁) = 1.8 + 3.1x demonstrates how least squares produces a line balancing proximity to all data points rather than passing through any specific point perfectly.\n",
    "\n",
    "## Vector and Matrix Formulation\n",
    "\n",
    "As models increase in complexity, involving multiple input attributes or higher-order terms, scalar approaches become unwieldy. Vector and matrix notation provides an elegant solution that scales to arbitrary complexity. Parameters combine into a parameter vector w = [w₀, w₁]ᵀ, and each input is augmented with a constant term: xₙ = [1, xₙ]ᵀ.\n",
    "\n",
    "This reformulation transforms the model into f(xₙ; w) = wᵀxₙ, a simple dot product. The loss function becomes L = (1/N)(t - Xw)ᵀ(t - Xw), where X contains all input vectors and t contains all target values. This matrix formulation leads to the general solution ŵ = (XᵀX)⁻¹Xᵀt, applicable regardless of parameter count or input transformation complexity.\n",
    "\n",
    "The approach's power becomes apparent when extending to multiple attributes. Predicting Olympic winning times using both year and personal best times of eight lane competitors creates a 10-dimensional parameter space with input vector x = [1, year, s₁, s₂, ..., s₈]ᵀ. The matrix formulation handles this complexity seamlessly without requiring methodology changes.\n",
    "\n",
    "## Non-Linear Extensions Through Basis Functions\n",
    "\n",
    "Linear models need not be limited to straight-line relationships. Transforming input variables through basis functions enables capturing non-linear patterns while maintaining computational advantages of linear parameter estimation. The most common transformation involves polynomial terms, including powers like x², x³, etc.\n",
    "\n",
    "A quadratic model uses the augmented input vector [1, x, x²]ᵀ, resulting in f(x; w) = w₀ + w₁x + w₂x². This model captures curved relationships while using the same least squares solution framework. The chapter demonstrates this with an eighth-order polynomial fitted to Olympic data, showing how higher-order models achieve lower training errors.\n",
    "\n",
    "The concept extends beyond polynomials to any set of basis functions. For Olympic data exhibiting periodic behavior, a model incorporating trigonometric functions might be appropriate: f(x; w) = w₀ + w₁x + w₂sin((x-a)/b). This approach captures complex, non-linear relationships within the linear modeling framework.\n",
    "\n",
    "## The Challenge of Overfitting\n",
    "\n",
    "A critical insight emerges when comparing models of different complexity: more complex models invariably fit training data better, but improved fit doesn't necessarily translate to better predictions. The eighth-order polynomial achieves a training loss of 0.459 compared to 1.358 for the linear model, yet its predictions appear unrealistic, especially when extrapolated beyond observed data ranges.\n",
    "\n",
    "Overfitting occurs when models become so complex that they memorize specific training data details rather than learning underlying patterns. The model becomes overspecialized to training examples and loses its ability to generalize to new, unseen data. Understanding this trade-off between fitting training data and maintaining predictive capability represents one of machine learning's most important concepts.\n",
    "\n",
    "## Model Selection Through Validation\n",
    "\n",
    "To address overfitting, the chapter introduces validation techniques estimating how well models will perform on unseen data. The simplest approach involves setting aside a data portion (validation set) and using it to test models trained on remaining data (training set). For the Olympic example, data from 1980 onwards serves as validation data, with earlier years used for training.\n",
    "\n",
    "This validation approach reveals that simpler models often perform better on unseen data despite achieving higher training errors. When comparing polynomial models of increasing order, training error decreases monotonically with model complexity, but validation error typically decreases initially then increases, creating a U-shaped curve identifying optimal model complexity.\n",
    "\n",
    "Cross-validation extends this concept by systematically rotating which data serves as the validation set. In K-fold cross-validation, data is divided into K equal blocks, with each block serving as validation data while models train on remaining K-1 blocks. Leave-one-out cross-validation represents the extreme case where each individual data point serves as a validation set, providing more robust performance estimates, especially with limited data.\n",
    "\n",
    "## Regularization as Complexity Control\n",
    "\n",
    "An alternative approach to preventing overfitting involves explicitly penalizing model complexity during optimization. Regularization achieves this by adding a penalty term to the loss function that grows with parameter magnitude. The most common form, Ridge regression or L2 regularization, modifies the loss function to L' = L + λwᵀw.\n",
    "\n",
    "The regularization parameter λ controls the trade-off between fitting training data accurately and maintaining model simplicity. When λ = 0, the standard least squares solution is recovered. As λ increases, optimization increasingly favors simpler models with smaller parameter values, even at the cost of slightly higher training error.\n",
    "\n",
    "| λ Value | Effect | Model Characteristics |\n",
    "|---------|--------|----------------------|\n",
    "| λ = 0 | No regularization | Standard least squares solution |\n",
    "| Low λ | Minimal penalty | Slight parameter shrinkage |\n",
    "| High λ | Strong penalty | Significant parameter shrinkage, smoother models |\n",
    "\n",
    "The regularized solution becomes ŵ = (XᵀX + NλI)⁻¹Xᵀt, where I represents the identity matrix. This modification \"shrinks\" parameter estimates toward zero, producing smoother, more generalizable models. The challenge lies in selecting appropriate λ values, again relying on validation techniques to balance complexity and performance.\n",
    "\n",
    "## Practical Implications and Limitations\n",
    "\n",
    "The Olympic sprint example illustrates both strengths and limitations of linear modeling. The linear trend makes intuitive sense and provides reasonable short-term predictions. However, extrapolating far into the future reveals model limitations: it eventually predicts impossible negative winning times and fails to account for physical limits on human performance.\n",
    "\n",
    "These limitations highlight important considerations when applying linear models. The linear assumption may be inappropriate for many real-world relationships. Predictions become increasingly unreliable as they move further from training data ranges. The model's simplicity, while computationally advantageous, may miss important patterns or relationships in data.\n",
    "\n",
    "Despite these limitations, linear modeling provides an essential foundation for machine learning. Its analytical solutions offer computational efficiency, its mathematical properties are well understood, and its interpretable parameters provide insight into modeled relationships. Many advanced machine learning techniques build upon these linear foundations, making mastery of these concepts crucial for understanding more complex methods.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "Linear modeling serves as both a practical tool for regression problems and a theoretical foundation for machine learning. The progression from simple line fitting through matrix formulations to regularization and validation provides a comprehensive introduction to key machine learning concepts. The Olympic sprint example effectively demonstrates how abstract mathematical concepts apply to real-world prediction problems while illustrating the importance of understanding model limitations and the dangers of overfitting.\n",
    "\n",
    "The chapter establishes that successful machine learning requires balancing model complexity with generalization capability, introducing fundamental concepts that extend far beyond linear models into the broader field of machine learning and statistical modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5638c5-8736-4910-a3ae-987f773e1358",
   "metadata": {},
   "source": [
    "# Machine Learning Study Guide\n",
    "\n",
    "This study guide is based on class notes and readings from *A First Course in Machine Learning*.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to Machine Learning\n",
    "\n",
    "**Definition:**  \n",
    "Machine Learning is the process of *training* a model to learn patterns from data (features, independent variables) and *inferring* outcomes (labels, dependent variables).\n",
    "\n",
    "- We try to approximate a function `y = f(x)` such that predictions `ŷ` are as close as possible to the true labels `y`.\n",
    "- Example: A simple linear function `y = ax + b`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Linear Models\n",
    "\n",
    "**Key Characteristics:**  \n",
    "1. Linear relationships only.  \n",
    "   - **Addition of variables**: `y = a1x1 + a2x2 + ... + anxn + b`.  \n",
    "   - **Multiplication by constants**: scaling is allowed.  \n",
    "2. No multiplication of variables with each other (no `x1 * x2`) and no exponents beyond 1.  \n",
    "\n",
    "**Non-Example:**  \n",
    "- Quadratic equation: `y = ax² + bx + c` → not linear because of exponent `2`.\n",
    "\n",
    "**Why Linear Models?**  \n",
    "- Simple to interpret.  \n",
    "- Computationally efficient.  \n",
    "- Often a good starting point before moving to more complex models.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Evaluating Linear Models\n",
    "\n",
    "We want the model’s predictions to be as close as possible to actual data points.  \n",
    "This is done by minimizing **loss functions**.\n",
    "\n",
    "### Mean Squared Loss (MSE)\n",
    "- Formula:  \n",
    "  \\[ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - ŷ_i)^2 \\]\n",
    "\n",
    "- **Steps:**  \n",
    "  1. Compute the error (residual) between prediction and true value.  \n",
    "  2. Square each error (to avoid negatives).  \n",
    "  3. Take the mean of squared errors.  \n",
    "\n",
    "- **Why Square?**  \n",
    "  - Avoids cancellation of positive and negative errors.  \n",
    "  - Penalizes large errors more heavily.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Understanding Optimization with Derivatives\n",
    "\n",
    "Minimizing MSE requires finding the **best parameters (weights)**.  \n",
    "We use calculus to identify where the loss function is minimized.\n",
    "\n",
    "### Critical Points from Derivatives\n",
    "1. **Saddle Point** – function goes down in one direction, up in another.  \n",
    "2. **Local Maximum** – highest point in a region (not useful for minimization).  \n",
    "3. **Local Minimum** – lowest point in a region (what we want).  \n",
    "\n",
    "- **1st Derivative (Gradient):** Identifies slope; zero at critical points.  \n",
    "- **2nd Derivative (Curvature):** Helps classify if it’s a min, max, or saddle.  \n",
    "\n",
    "### Gradient Descent\n",
    "- An iterative algorithm to minimize loss.  \n",
    "- Updates parameters in the opposite direction of the gradient.  \n",
    "- Step size controlled by a **learning rate**.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Notes from *A First Course in Machine Learning – Chapter 1*\n",
    "\n",
    "### Linear Modelling: A Least Squares Approach\n",
    "\n",
    "- **Goal:** Fit a straight line through data by minimizing squared differences between predicted and observed values.  \n",
    "- **Least Squares Method:**  \n",
    "  - Choose coefficients (weights) that minimize the total squared error.  \n",
    "- **Applications:**  \n",
    "  - Predicting continuous outcomes (e.g., house prices, exam scores).  \n",
    "- **Limitations:**  \n",
    "  - Sensitive to outliers.  \n",
    "  - Cannot capture non-linear relationships.  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key Takeaways\n",
    "\n",
    "- Machine learning is about approximating unknown functions from data.  \n",
    "- Linear models are simple but powerful tools.  \n",
    "- Mean Squared Error is the most common loss function for regression.  \n",
    "- Derivatives help us optimize models and find minima.  \n",
    "- Least Squares is the foundation of linear regression.  \n",
    "\n",
    "---\n",
    "\n",
    "## 7. Study Tips\n",
    "\n",
    "- Be able to explain **why linear models are linear**.  \n",
    "- Practice deriving MSE and computing it on small datasets.  \n",
    "- Understand the intuition behind gradient descent, not just the formula.  \n",
    "- Work through examples of fitting a line by hand using least squares.  \n",
    "- Contrast linear vs. non-linear models (when each is appropriate).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722912d-ec6e-4b75-b4b4-ce42397b6ca3",
   "metadata": {},
   "source": [
    "# Machine Learning Cheat Sheet\n",
    "\n",
    "## 1. Machine Learning Basics\n",
    "- Learn patterns (training), predict outcomes (inference).\n",
    "- Features = independent variables (x).  \n",
    "- Labels = dependent variables (y).  \n",
    "- Example: `y = ax + b`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Linear Models\n",
    "- Linear = addition + scaling only.  \n",
    "- No variable multiplication or exponents > 1.  \n",
    "- Form: `y = a1x1 + a2x2 + ... + anxn + b`.  \n",
    "- Non-linear: `y = ax² + bx + c`.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Loss Function (Regression)\n",
    "**Mean Squared Error (MSE):**  \n",
    "\\[ MSE = \\frac{1}{n} \\sum (y - ŷ)^2 \\]  \n",
    "- Squaring penalizes large errors.  \n",
    "- Minimization goal: smallest MSE.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Optimization & Derivatives\n",
    "- **1st Derivative = Gradient (slope).**  \n",
    "- **2nd Derivative = Curvature.**\n",
    "  - Saddle point: mix of up/down.  \n",
    "  - Local max: peak.  \n",
    "  - Local min: valley → best fit.  \n",
    "\n",
    "**Gradient Descent:** Iteratively updates weights in direction opposite gradient.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Least Squares (Linear Regression)\n",
    "- Choose coefficients that minimize squared errors.  \n",
    "- Foundation of regression.  \n",
    "- Good for continuous predictions (e.g., prices).  \n",
    "- Weakness: sensitive to outliers, cannot model non-linear patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key Reminders\n",
    "- Start simple: linear → more complex later.  \n",
    "- Outliers can distort linear regression.  \n",
    "- Learning rate in gradient descent controls speed & accuracy.  \n",
    "- Linear ≠ always best, but essential foundation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004a967-887a-45e5-81f2-06d265064cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
