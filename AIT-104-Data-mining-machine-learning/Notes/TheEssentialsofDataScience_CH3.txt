Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 1 of 41

3
Data Wrangling
When we begin with a new data science project we will begin with understanding the business
problem to be tackled. This includes ensuring all participants in the project understand the goals,
the success criteria, and how the results will be deployed into production in the business. We then
liaise with the business data technicians to identify the available data. This is followed by a data
phase where we work with the business data technicians to access and ingest the data into R. We
are then in a position to move on with our journey to the discovery of new insights driven by the
data. By living and breathing the data in the context of the business problem we gain our bearings
and feed our intuitions as we journey.
In this chapter we present and then capture a common series of steps that we follow as the data
phase of data science. As we progress through the chapter we will build a template * designed to
be reused for journeys through other datasets. As we foreshadowed in Chapter 2 rather than
delving into the intricacies of the R language we immerse ourselves into using R to achieve our
outcomes, learning more about R as we proceed.
As we will for each chapter we begin with loading the packages required for this chapter.
Packages used in this chapter include FSelector (Romanski and Kotthoff, 2016), dplyr (Wickham
et al., 2017a), ggplot2 (Wickham and Chang, 2016), lub- ridate (Grolemund et al., 2016),
randomForest (Breiman et al., 2015), rattle (Williams, 2017), readr (Wickham et al., 2017b),
scales (Wickham, 2016), stringi (Gagolewski et al., 2017), stringr (Wickham, 2017a), tibble
(Müller and Wickham, 2017), tidyr (Wickham, 2017b) and magrittr (Bache and Wickham, 2014).

3.1

Data Ingestion

To begin the data phase of a project we typically need to ingest data into R. For our purposes we
will ingest the data from the simplest of sources—a text-based CSV (comma separate value) file.
Practically any source format is supported by R through the many packages available. We can
find, for example, support for ingesting data directly from Excel spreadsheets or from database
Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 2 of 41

servers.The weatherAUS dataset from rattle will serve to illustrate the preparation of data for
modelling. Both a CSV file and an R dataset are provided by the package and the dataset is also
available directly from the Internet on the rattle web site. We will work with the CSV file as the
typical pathway for loading data into R.

Identifying the Data Source
We first identify and record the location of the CSV file to analyse. R is capable of loading data
directly from the Internet and so we will illustrate how to load the CSV file from the rattle web
site itself. The location of the file (the so-called URL or universal resource locator) will be saved
as a string of characters in a variable called dspath—the path to the dataset. This is achieved
through the following assignment which we type into our R script file within RStudio. The
command is then executed in RStudio by clicking the Run button whilst the cursor is located on
the line of code within the script file.

The assignment operator < - will store the value on the righthand side (the string of characters
enclosed within quotation marks) into the computer’s memory and we can later refer to it as the R
variable dspath—that is, we can retrieve the string simply by reference to the variable dspath.

Reading the Data
Having identified the source of the dataset we can read the dataset into the memory of the
computer using readr::read_csv(). This function returns a data frame (though it is actually an
enhanced data frame known as a table data frame ) which is the basic data structure used to store
a dataset within R. The data is stored as a table consisting of rows ( observations ) and columns (
variables ). We store the dataset (as a data frame) in the computer’s memory and reference it by
the R variable weatherAUS. It will then be ready to process using R.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 3 of 41

Template Variables
To support our goal of creating a reusable template we create a reference to the original dataset
using a template (or generic) variable. The new variable will be called ds (short for dataset).

Both ds and weatherAUS will initially reference the same dataset within the computer’s
memory. As we modify ds those modifications will only affect the data referenced by ds and not
the data referenced by weatherAUS. Effectively an extra copy of the dataset in the computer’s
memory will start to grow as we change the data from its original form. * From here on we no
longer refer to the dataset as weatherAUS but as ds. This allows the following steps to be
generic—turning the R code into a template requiring only minor modification when used with a
different dataset assigned into ds. Often we will find that we can simply load a different dataset
into memory, store it as ds and the following code remains essentially unchanged.
The next few steps of our template record the name of the dataset and a generic reference to the
dataset.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 4 of 41

We are a little tricky here in recording the dataset name in the variable dsname and then using
the function base::get() to make a copy of the original dataset reference and to link it to the
generic variable ds. We could simply assign the data to ds directly as we saw above. Either way
the generic variable ds refers to the same dataset. The use of base::get() allows us to be generic
within the template.
The use of generic variables within a template for the tasks we perform on each new dataset
will have obvious advantages but we need to be careful. A disadvantage is that we may be
working with several datasets and accidentally overwrite previously processed datasets referenced
using the same generic variable (ds). The processing of the dataset might take some time and so
accidentally losing it is not an attractive proposition. Be careful to manage the naming of datasets
appropriately to avoid any loss of processed data.

The Shape of the Data
Once the dataset is loaded we seek a basic understanding of the data—its shape. We are interested
in the size of the dataset in terms of the number of observations (rows) and variables (columns).
We can simply type the variable name that stores the dataset and will be presented with a
summary of the actual contents of the dataset.

The function base::dim() will provide dimension information (observations and variables) as
will base::nrow() and base::ncol(). We use rattle::comcat() to format numbers with commas.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 5 of 41

Data Ingestion
A useful alternative for our initial insight into the dataset is to use tibble::glimpse().

Normalizing Variable Names
Next we review the variables included in the dataset. The function base::names() will list the
names of the variables (columns).
Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 6 of 41

The names of the variables within the dataset as supplied to us may not be in any particular
standard form and may use different conventions. For example, we might see a mix of upper and
lower case letters (WindSpeed9AM) or variable names that are very long
(Wind_Speed_Recorded_Today_9am) or use sequential numbers to identify each variable (V004
or V010_wind_speed) or use codes (XVn34_windSpeed) or any number of other conventions.
Often we prefer and it is convenient to simplify the variable names to ease our processing and to
enforce a standard and consistent naming convention for ourselves. This will help us in
interacting the data without regular reference to how the variables are named.
A useful convention is to map all variable names to lowercase. R is case sensitive so that doing
this will result in different variable names as far as R is concerned. Such normalisation is useful
when different upper/lower case conventions are intermixed inconsistently in names like
Incm_tax_PyBl. Remembering how to capitalize when interactively exploring the data with
thousands of such variables can be quite a cognitive load. Yet we often see such variable names
arising in practise especially when we import data from databases which are often case insensitive.
We use rattle::normVarNames() to make a reasonable attempt of converting variables from a
dataset into a preferred standard form. The actual form follows the style we introduce in Chapter
11. The following example shows the original names and how they are transformed into a
normalized form. Here we make extensive use of the function base::names() to work with the
variable names. *
Notice the use of the assignment pipe here as introduced in Chapter 2. Recall that the magrittr::
% <> % operator will pipe the left-hand data to the function on the right-hand side and then
return the result to the left-hand side overwriting the original contents of the memory referred to
on the left-hand side. In this case, the left-hand side refers to the variable names of the dataset.

3.2

Data Review

Once we have loaded the dataset and had our initial review of its size and cleaned up the variable
names the next step is to understand its structure—that is, understand what the data within the
dataset looks like.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 7 of 41

Structure
A basic summary of the structure of the dataset is presented using tibble:: glimpse() as we saw
above.

From this summary we see the variable names, their data types and the first few values of the
variable. We can see a variety of data types here, ranging from Date (date), through character
(chr) and numeric (dbl).
We confirm the data looks as we would expect and begin to gain some insight into the data
itself. We might start asking questions such as whether the date values are a sequence of days as
we might expect. The first few locations are listed as Albury and so we might ask what the other
values are. We see minimum and maximum temperatures and we note the rainfall and
Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 8 of 41

evaporation. We expect each of these to be numeric though we observe that evaporation is
reported as a character variable (which we will come back to later). For the sample above we only
see missing values for evaporation. For the numerics we will want to understand the distributions
of the values of the variables.
Generally our datasets are very large, with many observations (often in the millions) and many
variables (sometimes in the thousands). We can’t be expected to browse through all of the
observations and variables. Instead we can review the contents of the dataset using utils::head()
and utils::tail() to review the top six (by default) and the bottom six observations.

We can also have a look at some random observations from the dataset to provide a little more
insight. Here we use dplyr::sample_n() to randomly select 6 rows from the dataset.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 9 of 41

All the time we are building a picture of the data we are looking at. We note the date appears to
be a daily sequence starting from December 2008 and ending in 2017. We also note that
evaporation is often but not always missing.

3.3

Data Cleaning

Identifying Factors
On loading the dataset into R we can see that a few variables have been identified as having
character (string of characters) as the values they store. Such variables are often called categoric
variables. Within R these are usually represented as a data type called factor and handled
specially by many of the modelling algorithms. Where the character data takes on a limited
number of possible values we will convert the variable from character into factor (categoric) so
as to take advantage of some special handling.
A factor is a variable that can only take on a specific number of known distinct values which
we call the levels of the factor. For datasets that we load into R we will not always have
examples of all levels of a factor. Consequently it is not always possible to automatically list all
of the levels required for the definition of a factor. Thus we load these variables by default as
character and then convert them to factor as required.
From our review of the data so far we start to make some observations about the character
variables. The first is location. We note that several locations were reported in the above
exploration of the dataset. We can confirm the number of locations by counting the number of
base::unique() values the variable has in the original dataset.

We may not know in general what other locations we will come across in related datasets and
we already have quite a collection of 49 locations. We might normally decide to retain this
variable as a character data type, but for illustrative purposes we will convert it to a factor.

We next review a base::table() of the distribution of the locations.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 10 of 41

Two related variables that are class character that might be better represented as factors are
rain_today and rain_tomorrow. We can review the distribution of their values with the following
code.

Here we dplyr::select() from the dataset those variables that start with the string rain_ and then
build a base::table() over those variables in the subset of the original dataset selected. We use
base::sapply() to apply base::table() to the selected columns since the function takes a single
column from the dataset as its argument. This function counts the frequency of the occurence of
each value of a variable within the dataset.
We confirm that No and Yes are the only values these two variables have and so it makes sense
to convert them both to factors. We will keep the ordering as alphabetic and so a simple call to
base::factor() will convert each variable from character to factor using base::lapply(). Note the
use of base::data.frame() and dplyr::tbl_df() to ensure the data is in the correct form to overwrite
the original columns in the dataset.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 11 of 41

We can again obtain a distribution of the variables to confirm that all we have changed is the
data type.

The three wind direction variables identified as wind_gust_dir, wind_dir_9am and
wind_dir_3pm are also character variables. We will want to review their distribution of values
and can do so in a similar way. Here we dplyr::select() from the dataset those variables containing
the string _dir and then build a base:: table() over those variables in the selected subset of the
original dataset. We again use base::sapply() with base::table() to count the frequency of the
occurence of each level of the factors within the dataset.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 12 of 41

From the table we notice 16 compass directions. All compass directions are represented and so
we will convert these character variables into factors. Notice that the values of the variables are
listed in alphabetic order in the above and a simple conversion to a factor will retain the
alphabetic order. We might know however that the compass orders the directions in a welldefined manner (from N, NNE, to NW and NNW). With this knowledge we will force the levels
to have the appropriate ordering and also let base::factor() know that the levels are ordered with
ordered=TRUE.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 13 of 41

Again we obtain a distribution of the variables to confirm that all we have changed is the data
type.

There are two other variables that have been identified as character data types: evaporation and
sunshine. If we look at the dataset we see they have missing values.
Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 14 of 41

The heuristic used to determine the data type when readr::read_csv() ingests the data only
looks at a subset of all the data before it determines the data types. In this case they were missing
for the early observations and so in the absence of further information they were represented as
character. We need to convert them to numeric.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 15 of 41

We have now dealt with all of the character variables converting them to factors or numerics on
a case-by-case basis on our understanding of the data.

Normalise Factors
Some variables will have levels with spaces, and mixture of cases, etc. We may like to normalise
the levels for each of the categoric variables. For very large datasets this can take some time and
so we may choose to be selective if there are many factors.

To confirm we can review the categoric variables.

Ensure Target is a Factor
Many data mining tasks can be expressed as building classification models. For such models we
want to ensure the target is categoric. Often it is 0/1 and hence is loaded as numeric. In such cases
we could tell our model algorithm to explicitly do classification or else set the target using base::
as.factor() in the formula. Nonetheless it is generally cleaner to do this here and note that this
code has no effect if the target is already categoric.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 16 of 41

It is always a good idea to visualise the distribution of the target (and other) variables using
ggplot2. We can pipe the dataset into ggplot2::ggplot() whereby the target is associated through
ggplot2::aes_string() (the aesthetics) with the x-axis of the plot. To this we add a graphics layer
using ggplot2::geom_bar() to produce the bar chart, with bars having width= 0.2 and a fill= color
of “grey”. The resulting plot can be seen in Figure 3.1. With some surprise we note that there are
missing values in the dataset. We will deal with the missing values (NA) shortly.

Figure 3.1 Target variable distribution. Plotting the distribution is useful to gain an insight into the number of
observations in each category. As is the case here we often see a skewed distribution.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

3.4

Ebook pages 47-64 | Printed page 17 of 41

Variable Roles

Now that we have a basic idea of the size and shape and contents of the dataset and have
performed some basic data type identification and cleaning, we are in a position to identify the
roles played by the variables within the dataset. We record the list of available variables so that
we might reference them shortly.

By this stage of the project we will usually have identified a business problem that is the focus
of attention. In our case we will assume it is to build a predictive analytics model to predict the
chance of it raining tomorrow given the observation of today’s weather. In this case, the variable
rain_tomorrow is the target variable. Given today’s observations of the weather this is what we
want to predict. The dataset we have is then a training dataset of historic observations. The task
in model building is to identify any patterns among the other observed variables that suggest that
it rains the following day.
We also take the opportunity here to move the target variable to be the first in the vector of
variables recorded in vars. This is common practice where the first variable in a dataset is the
target and the remainder are the variables that will be used to build a model. Another common
practise is for the target to be the final column of the dataset.

Notice the use of base::unique() simply to remove the original occurrence of the target variable.
Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 18 of 41

Another variable that we observe as relating to the outcome rather than to today’s observations
is risk_mm. From the business context we would learn that this records the amount of rain that
fell “tomorrow”. We refer to this as a risk variable. It is a measure of the impact or risk of the
outcome we are predicting (whether it rains tomorrow). The risk is an output variable and should
not be used as an input to the modelling—it is not an independent variable. In other circumstances
it might actually be treated as the target variable.

Finally from our observations so far we note that the variable date acts as an identifier as does
the variable location. Given a date and a location we have an observation of the remaining
variables. We note these two variables as identifiers. Identifiers would not usually be used as
independent variables for building predictive analytics models.

3.5

Feature Selection

We now move on to identifying variables (features or columns of the dataset) that are irrelevant
or inappropriate for modelling.

IDs and Outputs
We start by noting that we should ignore all identifiers and any risk variable (which will be an
output variable rather than an input variable). These variables should be ignored in our modelling.
Always watch out for treating output variables as inputs to modelling—this is a surprisingly
common trap for beginners.
We will build a vector of the names of the variables to ignore. Above we have already recorded
the id variables and (optionally) the risk. Here we join them into a new vector using dplyr::union()
which performs a set union operation—that is, it joins the two arguments together and removes
any repeated variables.

We might also check for variables that have a unique value for every observation. These are
often identifiers and if so they are candidates for ignoring.
We begin in the following code block by defining a helper function that given a vector of data
it will return the number of unique values found in that vector. This helper function is then

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 19 of 41

deployed in the following pipeline to identify those vars which have as many unique values as
there are rows in the dataset. The pipeline is explained next. This is our first example of defining
our own function.

We can step through this code line by line to understand its workings. To find the candidate
identifiers we retain just the vars from the dataset and pipe this subset of the original dataset ds
through to base::sapply(). The function base::sapply() applies a supplied function to every
column of the provided dataset. The function we supply is the helper function we defined. The (x)
represents a single column of the dataset at a time. Thus, for each column we identify the base::
unique() values in that column and then return the base::length() of the vector of unique values.
The base::sapply() sends the vector of lengths (the number of unique values for each of the
columns) on to the next operation through the pipeline. The follow-on operation tests if the
calculated number of unique values magrittr::equals() the number of rows in the dataset as
calculated using base::nrow(). The resulting vector of logical values is then piped to base::which()
to retain those that are TRUE—those that have as many unique values as there are rows in the
dataset.
Finally, we extract the base::names() of these variables and store them as the variable ids after
printing them for information purposes.
We have strung together a series of operations here with each operation piping data on to the
next operation. It is worth taking a little time to understand the sequence as a single sentence in
the grammar of data wrangling . As we interact with our data we typically build the sequence
adding one extra process at a time in the R Console, confirming the results as we go.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 20 of 41

The pipeline has identified no variables as potential identifiers themselves in this dataset;
hence, the character(0) result. Below we choose observations from a single location and process
that data through the above pipeline to illustrate the selection of date as having a unique value for
every observation.

All Missing
We next remove any variable where all of the values are missing. Our pipeline here counts the
number of missing values for each variable and then lists the names of those variables that have
no values. We introduce another small helper function to count the number of missing values for
a vector.

Again there are no variables that are completely missing in the weatherAUS dataset but in
general it is worth checking. Below we engineer a dataset with all missing values for some
variables to illustrate the pipeline in action.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 21 of 41

Note that it is also adding to our knowledge of this dataset that there are locations for which
some variables are not observed or recorded. This may play a role in understanding how to model
the data.

Many Missing
It is also useful to identify those variables which are very sparse— that have mostly missing
values. We can decide on a threshold of the proportion missing above which to ignore the
variable as not likely to add much value to our analysis. For example, we may want to ignore
variables with more than 80% of the values missing:

Here again we identify no variables that have a high proportion of missing observations.

Too Many Levels
Another issue we often come across in our datasets are factors that have very many levels. We
might want to ignore such variables (or perhaps group them appropriately). Here we simply
identify them and add them to the list of variables to ignore:
Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 22 of 41

The variable location is identified as having too many levels and is thus added to the ignore list
though since it is already on that list there is no change to it.

Constants
We should also ignore variables with constant values as they generally add no extra information
to the analysis.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 23 of 41

There are no constants found in this dataset.

Correlated Variables
It is often useful to reduce the number of variables we are modelling by identifying and removing
highly correlated variables. Such variables will often record the same information but in different
ways. Correlated variables can often arise when we combine data from different sources.
First we will identify the numeric variables on which we will calculate correlations. We start by
removing the ignored variables from the dataset. We then identify the numeric variables by base::
sapply()ing the function base::is.numeric() to the dataset then find base::which() variables are
numeric. The variable names are stored into the variable numc.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 24 of 41

We can then calculate the correlation between the numeric variables by selecting the numeric
columns from the dataset and passing that through to stats::cor(). This generates a matrix of
pairwise correlations based on only the complete observations so that observations with missing
values are ignored.
We set the upper triangle of the correlation matrix to NA’s as they are a mirror of the values in
the lower triangle and thus redundant. Notice that with diag=TRUE this includes the diagonals of
the matrix being set to NA as they will always be perfect correlations (1).
Next we ensure the values are positive using base::abs(). We also ensure we have a base::data.
frame() which we convert to a dplyr::tbl_df(). The dataset column names need to be reset
appropriately using magrittr::set_colnames(). We dplyr::mutate() the dataset by adding a new
column, then tidyr::gather() the dataset. Missing correlations are omitted using stats::na.omit().
Finally, the rows are dplyr:: arrange()’d with the highest absolute correlations appearing first.

This is quite a complex pipeline. It is worth taking time to understand the sequence as a single
sentence in the grammar of data wrangling. Importantly it should be noted that we build up such a
command sequence interactively, adding one new command in the pipeline sequence at a time
until we have our final desired outcome. It is strongly recommended that you replicate building
the sequence one step at a time to review and understand the result after each step.
From the final result we can identify pairs of variables where we might want to keep one but
not the other variable because they are highly correlated. We will select them manually since it is
a judgement call. Normally we might limit the removals to those correlations that are 0.90 or
more. We should confirm that the three most highly correlated variables here make intuitive sense.
Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 25 of 41

Removing Variables
Once we have identified all of the variables to ignore we remove them from our list of variables
to use.

Algorithmic Feature Selection
There are many R packages available to support the preparation of our datasets and over time you
will find packages that suit your needs. As you do so they can be added to your version of the
template for data wrangling. For example, a useful package is FSelector which provides functions
to identify subsets of variables that might be most effective for modelling. We can use this (and
other packages) to further assist us in reducing the variables for modelling.
As an example we can use FSelector::cfs() to identify a subset of variables to consider for use
in modelling by using correlation and entropy.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 26 of 41

Notice the use of the stringi::%s+% operator as a convenience to concatenate strings together
to produce a formula that indicates we will model the target variable using all of the other
variables of the dataset.
A second example lists the variable importance using FSelector::information.gain() to advise
a useful subset of variables.

The two measures are somewhat consistent in this case. The variables identified by FSelector::
cfs() are mostly the more important variables identified by FSelector::information.gain().
However rain_today is a little further down on the information gain list.

3.6

Missing Data

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 27 of 41

A common task is to deal with missing values. Here we remove observations with a missing
target. As with any missing data we should also analyse whether there is any pattern to the
missing targets. This may be indicative of a systematic data issue rather than simple randomness.
It is important to investigate further why the data is systematically missing. Often this will also
lead to a better understanding of the data and how it was collected.

Missing values in the input variables are also an issue for some but not all algorithms. For
example, the traditional ensemble model building algorithm randomForest::randomForest()
omits observations with missing values by default whilst rpart::rpart() has a particularly welldeveloped approach to dealing with missing values.
In the previous section we removed variables with many missing values noting that this is not
always appropriate. We may want to instead impute missing values in the data (noting also that it
is not always wise to do so as it is effectively inventing data).
Here we illustrate a process for imputing missing values using randomForest::na.roughfix().
As the name suggests, this function provides a basic algorithm for imputing missing values. We
will demonstrate the process but then restore the original dataset rather than have the imputations
included in our actual dataset.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 28 of 41

An alternative might be to remove observations that have missing values. We use stats::na.
omit() to identify the rows to omit based on the vars to be included for modelling. The list of
rows to omit is stored as the na.action attribute of the returned object. We can then remove these
observations from the dataset. We start again by keeping a copy of the original dataset to restore
later. We also initialise a list of row indicies that we will (omit) from the dataset.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 29 of 41

By restoring the dataset to its original contents to continue with our analysis we are deciding
not to omit any observations at this time.

3.7

Feature Creation

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 30 of 41

Another important task for the data scientist is to create new features from the provided data
where appropriate. We will illustrate with two examples. Note that we will often iterate over the
feature creation process many times during the life cycle of a project. New derived features will
become identified as we gain insights into the data and through our modelling.

Derived Features
From a review of the data we note that each observation has a date associated with it. Unless we
are specifically performing time series analysis (and indeed this would be an appropriate analysis
to consider for this dataset) some derived features may be useful for our model building rather
than using the specific dates as input variables.
Here we add two derived features to our dataset: year and season. The decision to add these
was made after we initially began exploring the data and building our initial predictive models.
Feedback from our domain expert suggested that the changing pattern over the years is of interest
and that predicting rain will often involve seasonal adjustments.
The dataset is dplyr::mutate()’d to add these two features. The year is a simple extraction from
the date using base::format(). To compute the season we extract the month base::as.integer()
which is then an index to base::switch() to a specific season depending on the month.

The final line of code prints a random sample of the original and new features. It is critical to
confirm the results are as expected—a sanity check.
The introduced variables will have different roles which are recorded appropriately.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 31 of 41

Model-Generated Features
In addition to developing features derived from other features we will sometimes find it useful to
include model-generated features. A common one is to cluster observations or some aggregation
of observations into similar groups. We then uniquely identify each group (for example, by
numbering each group) and add this grouping as another column or variable within our dataset.
A cluster analysis (also called segmentation) provides a simple mechanism to identify groups
and allows us to then visualise within those groups, for example, rather than trying to visualise
the whole dataset at one time. We will illustrate the process of a cluster analysis over the numeric
variables within our dataset. The aim of the cluster analysis is to group the locations so that within
a group the locations will have similar values for the numeric variables and between the groups
the numeric variables will be more dissimilar. The traditional clustering algorithm is stats::
kmeans().
In the following code block we specify the number of clusters that we wish to create, storing
that as the variable NCLUST. We then select the numc (numeric) variables from the dataset ds,
dplyr::group_by() the location and then dplyr::summarise_all() variables. As an interim step we
store for later use the names of the locations. We then continue by removing location from the
dataset leaving just the numeric variables. The variables are then rescaled so that all values across
the different variables are in the same range. This rescaling is required for cluster analysis so as
not to introduce bias due to very differently scaled variables. The stats::kmeans() cluster analysis
is then performed. We base::print() the results and magrittr::extract2() just the cluster number
for each location.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 32 of 41

The cluster numbers are now associated with each location as stored within the vector using
their base::names(). We can then dplyr::mutate() the dataset by adding the new cluster number
indexed by the location for each observation.

The introduced variable’s role in modelling is recorded appropriately.

A quick sanity check will indicate that basically the clustering looks okay, grouping locations
that the domain expert agrees are similar in terms of weather patterns.

3.8

Preparing the Metadata

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 33 of 41

Metadata is data about the data. We now record data about our dataset that we use later in further
processing and analysis.

Variable Types
We identify the variables that will be used to build analytic models that provide different kinds of
insight into our data. Above we identified the variable roles such as the target, a risk variable and
the ignored variables. From an analytic modelling perspective we also identify variables that are
the model inputs (also called the independent variables). We record then both as a vector of
characters (the variable names) and a vector of integers (the variable indicies).

The integer indices are determined from the base::names() of the variables in the original
dataset. Note the use of USE.NAMES= from base::sapply() to turn off the inclusion of names in
the resulting vector to keep the result as a simple vector.

For convenience we also record the number of observations:

Next we report on the dimensions of various data subsets primarily to confirm that the dataset
appears as we expect:

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 34 of 41

Sometimes we need to identify the numeric and categoric variables for separate handling.
Many cluster analysis algorithms, for example, only deal with numeric variables. Here we
identify them both by name (a character string) and by index. Note that when using the index we
have to assume the variables remain in the same order within the dataset and all variables are
present. Otherwise the indicies will get out of sync.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

3.9

Ebook pages 47-64 | Printed page 35 of 41

Preparing for Model Building

Our end goal is to build a model based on the data we have prepared. A model will capture
knowledge about the world that the data represents. The final two tasks in preparing our data for
modelling are to specify the form of the model to be built and to identify the actual observations
from which the model is to be built. For the latter task we will partition the dataset into three
Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 36 of 41

subsets. Whilst this is primarily useful for model building, it may sometimes be useful to explore
a random subset of the whole dataset so that our interactions are more interactive, particularly
when dealing with large datasets.

Formula to Describe the Model
A formula is used to identify what it is that we will model from the supplied data. Typically we
identify a target variable which we will model based on other input variables . We can construct
a stats:: formula() automatically from a dataset if the first column of the dataset is the target
variable and the remaining columns are the input variables. A simple selection of the columns in
this order will generate the initial formula automatically. Earlier we engineered the variable vars
to be in the required order.

The notation used to express the formula begins with the name of a target (rain_tomorrow)
followed by a tilde (~) followed by the variables that will be used to model the target, each
separated by a plus (+). The formula here indicates that we aim to build a model that captures the
knowledge required to predict the outcome rain_tomorrow from the provided input variables
(details of today’s weather). This kind of model is called a classification model and can be
compared to regression models, for example, which predict numeric outcomes. Specifically, with
just two values to be predicted, we call this binary classification. It is generally the simplest kind
of modelling task but also a very common task.

Training, Validation and Testing Datasets
Models are built using a machine learning algorithm which learns from the dataset of historic
observations. A common methodology for building models is to partition the available data into a
training dataset and a testing dataset . We will build (or train ) a model using the training dataset
and then test how good the model is by applying it to the testing dataset. Typically we also
introduce a third dataset called the validation dataset . This is used during the building of the
model to assist in tuning the algorithm through trialling different parameters to the machine
learning algorithm. In this way we search for the best model using the validation dataset and then
obtain a measure of the performance of the final model using the testing dataset.
The original dataset is partitioned randomly into the three subsets. To ensure we can repeatably
reproduce our results we will first initiate a random number sequence with a randomly selected
seed. In this way we can replicate the examples presented in this book by ensuring the same
random subset is selected each time. We will initialise the random number generator with a
specific seed using base::set.seed(). For no particular reason we choose 42.
Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 37 of 41

We are now ready to partition the dataset into the two or three subsets. The first is typically a
70% random sample for building the model (the training dataset). The second and third consist of
the remainder, used to tune and then estimate the expected performance of the model (the
validation and testing datasets).
Rather than actually creating three subsets of the dataset, we simply record the index of the
observations that belong to each of the three subsets.

We take a moment here to record the actual target values across the three datasets as these will
be used in the evaluations performed in later chapters. A secondary target variable is noted,
referred to as the risk variable. This is a measure of the impact or size of the outcome and is
common in insurance, financial risk and fraud analysis. For our sample dataset, the risk variable is
risk_mm which reports the amount of rain recorded tomorrow.
Notice the correspondence between risk values (the amount of rain) and the target, where 0.0
mm of rain corresponds to the target value no. In particular though also note that small amounts
of rain (e.g., 0.2 mm and 0.4 mm) are treated as no rain.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

3.10

Ebook pages 47-64 | Printed page 38 of 41

Save the Dataset

Having transformed our dataset in a variety of ways, cleaned it, wrangled it, and added new
variables, we will now save the data and its metadata into a binary RData file. Saving it as a
binary compressed file saves both storage space and time on reloading the dataset. Loading a
binary dataset is generally faster than loading a CSV file.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 39 of 41

Notice that in addition to the dataset (ds) we also store the collection of metadata . This begins
with items such as the name of the dataset, the source file path, the date we obtained the dataset,
the number of observations, the variables of interest, the target variable, the name of the risk
variable (if any), the identifiers, the variables to ignore and observations to omit. We continue
with the indicies of the input variables and their names, the indicies of the numeric variables and
their names, and the indicies of the categoric variables and their names.
Each time we wish to use the dataset we can now simply base::load() it into R. The value that
is invisibly returned by base::load() is a vector naming the R objects loaded from the binary
RData file.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 40 of 41

A call to base:: load() returns its result invisibly since we are primarily interested in its sideeffect. The side-effect is to read the R binary data from storage and to make it available within
our current R session.

3.11

A Template for Data Preparation

Throughout this chapter we have worked towards constructing a standard template for ourselves
for a data preparation report. Indeed, whenever we begin a new project the template will provide
a useful starting point. The introduction of generic variables facilitates this approach to quickly
begin any new analysis.
A template based on this chapter for data preparation is available from https://essentials.
togaware.com. There we can find a template that will continue to be refined over time and will
incorporate improvements and advanced techniques that go beyond what has been presented here.
An automatically derived version including just the R code is available there together with the L AT
EX compiled PDF version.
Notice that we would not necessarily perform all of the steps we have presented in this chapter.
Normalizing the variable names or imputing imputing missing values, omitting observations with
missing values, and so on may well depend on the context of the analysis. We will pick and
choose as is appropriate to our situation and specific datasets. Also, some data-specific
transformations are not included in the template and there may be other transforms we need to
perform that we have not covered here.

3.12

Exercises

Exercise 3.1 Exploring the Weather
We have worked with the Australian weatherAUS dataset throughout this chapter. For this
exercise we will explore the dataset further.
1. Create a data preparation script beginning with the template available from
https://essentials.togaware.com and replicate the data processing performed in this
chapter.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

Copyright © 2017. CRC Press LLC. All rights reserved.

Ebook pages 47-64 | Printed page 41 of 41

2. Investigate the dplyr::group_by() and dplyr::summarise() functions, combined
through a pipeline using magrittr::% > % to identify regions with considerable
variance in their weather observations. The use of stats::var() might be a good starting
point.

Exercise 3.2 Understanding Ferries
A dataset of ferry crossings on Sydney Harbour is available as https://essentials.togaware.com
/ferry.csv. * We will use this dataset to exercise our data template.
1. Create a data preparation script beginning with the template available from
https://essentials.togaware.com/data.R.
2. Change the sample source dataset within the template to download the ferry dataset
into R.
3. Rename the variables to become normalized variable names.
4. Create two new variables from sub_route, called origin and destination.
5. Convert dates as appropriate and convert other character variables into factors where
it is sensible to do so.
6. Work through the template to prepare and then explore the dataset to identify any
issues and to develop initial observations.
Create a report on your investigations and share a narrative to communicate your discoveries
from the dataset.

Footnote
* The template will consist of programming code that can be reused with little or no modification on a new
dataset. The intention is that to get started with a new dataset only a few lines at the top of the template
need to be
modified.
* R avoids making copies of datasets unnecessarily and so a simple assignment does not create a new copy.
As modifications are made to one or the other copy of a dataset then extra memory will be used to store
the columns that differ between the datasets.
* When the name of a variable within a dataset is changed a new copy of that value of that variable is created
so that now ds and weatherAUS will now be referring to different datasets in memory.
* The original source of the Ferry dataset is https://www.bts.nsw.gov.au/ Statistics/Ferry/default.aspx?
FolderID=224. The dataset is available under a Creative Commons Attribution (CC BY 3.0 AU) license.

Williams, Graham J.. The Essentials of Data Science: Knowledge Discovery Using R, CRC Press LLC, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/gcu/detail.action?docID=4929754.
Created from gcu on 2025-09-25 16:49:11.

